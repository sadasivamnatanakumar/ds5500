{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc3d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import webvtt\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a955b0bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found subtitle file: manual_sub_copy/Lecture12TheRatioRootandAlternatingSeriesTests_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (57920598,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture12TheRatioRootandAlternatingSeriesTests.wav with 661 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture9LimsupLiminfandtheBolzano-WeierstrassTheorem_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (70756011,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture9LimsupLiminfandtheBolzano-WeierstrassTheorem.wav with 987 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture8TheSqueezeTheoremandOperationsInvolvingConvergentSequences_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (71873877,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture8TheSqueezeTheoremandOperationsInvolvingConvergentSequences.wav with 923 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture10TheCompletenessoftheRealNumbersandBasicPropertiesofInfiniteSeries_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (72580437,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture10TheCompletenessoftheRealNumbersandBasicPropertiesofInfiniteSeries.wav with 921 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture15TheContinuityofSineandCosineandtheManyDiscontinuitiesofDirichletsFunction_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (59486180,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture15TheContinuityofSineandCosineandtheManyDiscontinuitiesofDirichletsFunction.wav with 738 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture7ConvergentSequencesofRealNumbers_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (58215083,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture7ConvergentSequencesofRealNumbers.wav with 757 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture11AbsoluteConvergenceandtheComparisonTestforSeries_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (57640844,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture11AbsoluteConvergenceandtheComparisonTestforSeries.wav with 637 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture13LimitsofFunctions_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (69971940,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture13LimitsofFunctions.wav with 910 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture6TheUncountabalityoftheRealNumbers_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (78408704,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture6TheUncountabalityoftheRealNumbers.wav with 972 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture14LimitsofFunctionsinTermsofSequencesandContinuity_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (58770263,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture14LimitsofFunctionsinTermsofSequencesandContinuity.wav with 690 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture16TheMinMaxTheoremandBolzanosIntermediateValueTheorem_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (65663803,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture16TheMinMaxTheoremandBolzanosIntermediateValueTheorem.wav with 734 entries.\n",
      "Combined dataset created with 11 individual datasets.\n"
     ]
    }
   ],
   "source": [
    "def vtt_to_text(vtt_file):\n",
    "    transcript = []\n",
    "    for caption in webvtt.read(vtt_file):\n",
    "        transcript.append((caption.start, caption.end, caption.text.strip()))\n",
    "    return transcript\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(audio_file, transcript_data, target_sample_rate=16000):\n",
    "    try:\n",
    "        #Load the audio file with scipy\n",
    "        sample_rate, audio_data = wavfile.read(audio_file)\n",
    "        print(f\"Audio data shape: {audio_data.shape}, Sample rate: {sample_rate}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio file {audio_file}: {e}\")\n",
    "\n",
    "    #Preparing the dataset\n",
    "    data = []\n",
    "    \n",
    "    for start, end, text in transcript_data:\n",
    "        #Converting time from HH:MM:SS to seconds\n",
    "        start_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(start.split(\":\"))))\n",
    "        end_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(end.split(\":\"))))\n",
    "        \n",
    "        #Calculating start and end frames based on the target sample rate\n",
    "        start_frame = max(0, min(int(start_sec * sample_rate), len(audio_data)))\n",
    "        end_frame = max(0, min(int(end_sec * sample_rate), len(audio_data)))\n",
    "\n",
    "        #Cutting the audio chunk for the corresponding transcript\n",
    "        audio_chunk = audio_data[start_frame:end_frame]\n",
    "\n",
    "        #Ensure the audio chunk is in the correct format\n",
    "        audio_chunk_list = audio_chunk.tolist()\n",
    "        data.append({\"audio\": audio_chunk_list, \"text\": text})\n",
    "\n",
    "    #Check if transcript data is empty\n",
    "    if not data:\n",
    "        print(f\"No valid transcript data for {audio_file}.\")\n",
    "        return None\n",
    "    \n",
    "    #Creating the dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"audio\": [d[\"audio\"] for d in data],\n",
    "        \"text\": [d[\"text\"] for d in data]\n",
    "    })\n",
    "    \n",
    "    print(f\"Created dataset for {audio_file} with {len(data)} entries.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def find_subtitle_file(audio_filename, subtitle_folder):\n",
    "    base_audio_name = os.path.splitext(audio_filename)[0]  \n",
    "    # Searching for the subtitle file that contains the audio filename in their name\n",
    "    for subtitle_filename in os.listdir(subtitle_folder):\n",
    "        if base_audio_name in subtitle_filename and subtitle_filename.endswith('.vtt'):\n",
    "            return os.path.join(subtitle_folder, subtitle_filename)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def process_videos(audio_folder, subtitle_folder):\n",
    "    datasets = []\n",
    "\n",
    "    for audio_file in os.listdir(audio_folder):\n",
    "        if audio_file.endswith('.wav'):\n",
    "            input_file_path = os.path.join(audio_folder, audio_file)\n",
    "\n",
    "            #Find the corresponding subtitle file\n",
    "            subtitle_file_path = find_subtitle_file(audio_file, subtitle_folder)\n",
    "            if subtitle_file_path:\n",
    "                print(f\"Found subtitle file: {subtitle_file_path}\")\n",
    "\n",
    "                #Convert subtitles to text transcript\n",
    "                transcript_data = vtt_to_text(subtitle_file_path)\n",
    "                if transcript_data:\n",
    "                    #Create the dataset using the existing WAV file\n",
    "                    dataset = create_dataset(input_file_path, transcript_data)\n",
    "                    if dataset:  #Check if the dataset is created\n",
    "                        datasets.append(dataset)\n",
    "                    else:\n",
    "                        print(f\"Failed to create dataset for {input_file_path}.\")\n",
    "                else:\n",
    "                    print(f\"No transcript data found for {subtitle_file_path}. The file may be empty or incorrectly formatted.\")\n",
    "            else:\n",
    "                print(f\"Subtitle file not found for {audio_file}. Expected naming convention: {os.path.splitext(audio_file)[0]}*.vtt\")\n",
    "\n",
    "    #Concatenate all datasets\n",
    "    if datasets:\n",
    "        combined_dataset = concatenate_datasets(datasets)\n",
    "        print(f\"Combined dataset created with {len(datasets)} individual datasets.\")\n",
    "        return combined_dataset\n",
    "    else:\n",
    "        print(\"No datasets were created.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "audio_path = 'audio_files_copy/'\n",
    "subtitle_path = 'manual_sub_copy/'\n",
    "combined_dataset = process_videos(audio_path, subtitle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5945bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2fa33e4ce847f5b1d07ff843d9908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load the processor for Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Function to preprocess each entry in the dataset\n",
    "def preprocess(batch):\n",
    "    # Convert audio (list of samples) to input features (Mel spectrograms)\n",
    "    audio = np.array(batch['audio'], dtype=np.float32)\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
    "    \n",
    "    # Tokenize the transcription text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "processed_dataset = combined_dataset.map(preprocess, remove_columns=[\"audio\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a62dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 8930\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4792c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c865e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_n = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbb98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = processed_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3abc2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataset: 8930\n",
      "Dataset shape: (8930, 2)\n",
      "Features of the dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Number of entries in the train dataset: 7144\n",
      "Train dataset shape: (7144, 2)\n",
      "Features of the train dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Number of entries in the eval dataset: 1786\n",
      "Eval dataset shape: (1786, 2)\n",
      "Features of the eval dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of entries in the dataset:\", len(processed_dataset))\n",
    "print(\"Dataset shape:\", processed_dataset.shape)\n",
    "print(\"Features of the dataset:\", processed_dataset.features)\n",
    "\n",
    "\n",
    "print(\"Number of entries in the train dataset:\", len(train_dataset))\n",
    "print(\"Train dataset shape:\", train_dataset.shape)\n",
    "print(\"Features of the train dataset:\", train_dataset.features)\n",
    "\n",
    "\n",
    "print(\"Number of entries in the eval dataset:\", len(eval_dataset))\n",
    "print(\"Eval dataset shape:\", eval_dataset.shape)\n",
    "print(\"Features of the eval dataset:\", eval_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a946d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='223' max='223' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [223/223 5:57:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>0.596659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.480200</td>\n",
       "      <td>0.404066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.349547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.323213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=223, training_loss=1.0463731534812482, metrics={'train_runtime': 21542.549, 'train_samples_per_second': 0.332, 'train_steps_per_second': 0.01, 'total_flos': 4.6284093259776e+17, 'train_loss': 1.0463731534812482, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "# For clearing MPS memory (macbook issues lol)\n",
    "torch.mps.empty_cache()  \n",
    "# Load the Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    #To use cpu since gpu memory was exceeded - mac issues again\n",
    "    no_cuda=True,  \n",
    "    num_train_epochs=1,  \n",
    "    save_steps=500,  \n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=False, \n",
    "    gradient_accumulation_steps=4,  \n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator_n,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b772abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and processor\n",
    "model.save_pretrained(\"./whisper-finetuned/final-model\")\n",
    "processor.save_pretrained(\"./whisper-finetuned/final-model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b214ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
