{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc3d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import webvtt\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq, pipeline\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a955b0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found subtitle file: manual_sub_copy/Lecture12TheRatioRootandAlternatingSeriesTests_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (57920598,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture12TheRatioRootandAlternatingSeriesTests.wav with 661 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture9LimsupLiminfandtheBolzano-WeierstrassTheorem_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (70756011,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture9LimsupLiminfandtheBolzano-WeierstrassTheorem.wav with 987 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture8TheSqueezeTheoremandOperationsInvolvingConvergentSequences_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (71873877,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture8TheSqueezeTheoremandOperationsInvolvingConvergentSequences.wav with 923 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture10TheCompletenessoftheRealNumbersandBasicPropertiesofInfiniteSeries_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (72580437,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture10TheCompletenessoftheRealNumbersandBasicPropertiesofInfiniteSeries.wav with 921 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture15TheContinuityofSineandCosineandtheManyDiscontinuitiesofDirichletsFunction_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (59486180,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture15TheContinuityofSineandCosineandtheManyDiscontinuitiesofDirichletsFunction.wav with 738 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture7ConvergentSequencesofRealNumbers_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (58215083,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture7ConvergentSequencesofRealNumbers.wav with 757 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture11AbsoluteConvergenceandtheComparisonTestforSeries_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (57640844,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture11AbsoluteConvergenceandtheComparisonTestforSeries.wav with 637 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture13LimitsofFunctions_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (69971940,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture13LimitsofFunctions.wav with 910 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture6TheUncountabalityoftheRealNumbers_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (78408704,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture6TheUncountabalityoftheRealNumbers.wav with 972 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture14LimitsofFunctionsinTermsofSequencesandContinuity_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (58770263,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture14LimitsofFunctionsinTermsofSequencesandContinuity.wav with 690 entries.\n",
      "Found subtitle file: manual_sub_copy/Lecture16TheMinMaxTheoremandBolzanosIntermediateValueTheorem_manual.en-j3PyPqV-e1s.vtt\n",
      "Audio data shape: (65663803,), Sample rate: 16000\n",
      "Created dataset for audio_files_copy/Lecture16TheMinMaxTheoremandBolzanosIntermediateValueTheorem.wav with 734 entries.\n",
      "Combined dataset created with 11 individual datasets.\n"
     ]
    }
   ],
   "source": [
    "def vtt_to_text(vtt_file):\n",
    "    transcript = []\n",
    "    for caption in webvtt.read(vtt_file):\n",
    "        transcript.append((caption.start, caption.end, caption.text.strip()))\n",
    "    return transcript\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(audio_file, transcript_data, target_sample_rate=16000):\n",
    "    try:\n",
    "        #Load the audio file with scipy\n",
    "        sample_rate, audio_data = wavfile.read(audio_file)\n",
    "        print(f\"Audio data shape: {audio_data.shape}, Sample rate: {sample_rate}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio file {audio_file}: {e}\")\n",
    "\n",
    "    #Preparing the dataset\n",
    "    data = []\n",
    "    \n",
    "    for start, end, text in transcript_data:\n",
    "        #Converting time from HH:MM:SS to seconds\n",
    "        start_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(start.split(\":\"))))\n",
    "        end_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(end.split(\":\"))))\n",
    "        \n",
    "        #Calculating start and end frames based on the target sample rate\n",
    "        start_frame = max(0, min(int(start_sec * sample_rate), len(audio_data)))\n",
    "        end_frame = max(0, min(int(end_sec * sample_rate), len(audio_data)))\n",
    "\n",
    "        #Cutting the audio chunk for the corresponding transcript\n",
    "        audio_chunk = audio_data[start_frame:end_frame]\n",
    "\n",
    "        #Ensure the audio chunk is in the correct format\n",
    "        audio_chunk_list = audio_chunk.tolist()\n",
    "        data.append({\"audio\": audio_chunk_list, \"text\": text})\n",
    "\n",
    "    #Check if transcript data is empty\n",
    "    if not data:\n",
    "        print(f\"No valid transcript data for {audio_file}.\")\n",
    "        return None\n",
    "    \n",
    "    #Creating the dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"audio\": [d[\"audio\"] for d in data],\n",
    "        \"text\": [d[\"text\"] for d in data]\n",
    "    })\n",
    "    \n",
    "    print(f\"Created dataset for {audio_file} with {len(data)} entries.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def find_subtitle_file(audio_filename, subtitle_folder):\n",
    "    base_audio_name = os.path.splitext(audio_filename)[0]  \n",
    "    # Searching for the subtitle file that contains the audio filename in their name\n",
    "    for subtitle_filename in os.listdir(subtitle_folder):\n",
    "        if base_audio_name in subtitle_filename and subtitle_filename.endswith('.vtt'):\n",
    "            return os.path.join(subtitle_folder, subtitle_filename)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def process_videos(audio_folder, subtitle_folder):\n",
    "    datasets = []\n",
    "\n",
    "    for audio_file in os.listdir(audio_folder):\n",
    "        if audio_file.endswith('.wav'):\n",
    "            input_file_path = os.path.join(audio_folder, audio_file)\n",
    "\n",
    "            #Find the corresponding subtitle file\n",
    "            subtitle_file_path = find_subtitle_file(audio_file, subtitle_folder)\n",
    "            if subtitle_file_path:\n",
    "                print(f\"Found subtitle file: {subtitle_file_path}\")\n",
    "\n",
    "                #Convert subtitles to text transcript\n",
    "                transcript_data = vtt_to_text(subtitle_file_path)\n",
    "                if transcript_data:\n",
    "                    #Create the dataset using the existing WAV file\n",
    "                    dataset = create_dataset(input_file_path, transcript_data)\n",
    "                    if dataset:  #Check if the dataset is created\n",
    "                        datasets.append(dataset)\n",
    "                    else:\n",
    "                        print(f\"Failed to create dataset for {input_file_path}.\")\n",
    "                else:\n",
    "                    print(f\"No transcript data found for {subtitle_file_path}. The file may be empty or incorrectly formatted.\")\n",
    "            else:\n",
    "                print(f\"Subtitle file not found for {audio_file}. Expected naming convention: {os.path.splitext(audio_file)[0]}*.vtt\")\n",
    "\n",
    "    #Concatenate all datasets\n",
    "    if datasets:\n",
    "        combined_dataset = concatenate_datasets(datasets)\n",
    "        print(f\"Combined dataset created with {len(datasets)} individual datasets.\")\n",
    "        return combined_dataset\n",
    "    else:\n",
    "        print(\"No datasets were created.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "audio_path = 'audio_files_copy/'\n",
    "subtitle_path = 'manual_sub_copy/'\n",
    "combined_dataset = process_videos(audio_path, subtitle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5945bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04906e925e8447087e54ff3ad0201fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dcc9cdb4f34f7d83fb4542e6f1a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239b47fab4fa4febaadfa1e623bba686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f386822a3248c88f2c8384ed0c972a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914433be472d44578bce742479bb9508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5f45cd2a0c4b4cbc857efe3ef42db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d6b7d79dd54846be492a9da20df5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f9a3649d8d481297f4398240bc7f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bafa2bd74c4ee5a84dcc1aa19a9964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load the processor for Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
    "\n",
    "# Function to preprocess each entry in the dataset\n",
    "def preprocess(batch):\n",
    "    # Convert audio (list of samples) to input features (Mel spectrograms)\n",
    "    audio = np.array(batch['audio'], dtype=np.float32)\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
    "    \n",
    "    # Tokenize the transcription text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "processed_dataset = combined_dataset.map(preprocess, remove_columns=[\"audio\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a62dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 8930\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4792c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's appended later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c865e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_n = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbb98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = processed_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3abc2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataset: 8930\n",
      "Dataset shape: (8930, 2)\n",
      "Features of the dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Number of entries in the train dataset: 7144\n",
      "Train dataset shape: (7144, 2)\n",
      "Features of the train dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Number of entries in the eval dataset: 1786\n",
      "Eval dataset shape: (1786, 2)\n",
      "Features of the eval dataset: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of entries in the dataset:\", len(processed_dataset))\n",
    "print(\"Dataset shape:\", processed_dataset.shape)\n",
    "print(\"Features of the dataset:\", processed_dataset.features)\n",
    "\n",
    "\n",
    "print(\"Number of entries in the train dataset:\", len(train_dataset))\n",
    "print(\"Train dataset shape:\", train_dataset.shape)\n",
    "print(\"Features of the train dataset:\", train_dataset.features)\n",
    "\n",
    "\n",
    "print(\"Number of entries in the eval dataset:\", len(eval_dataset))\n",
    "print(\"Eval dataset shape:\", eval_dataset.shape)\n",
    "print(\"Features of the eval dataset:\", eval_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a946d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df98b42e073a4b38b0d04b805a4fe47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a4d6442bab4eaaa33005e229585c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5a47d121114010967bd214918a2516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1281: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='223' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13/223 27:31 < 8:45:26, 0.01 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "# For clearing MPS memory (macbook issues lol)\n",
    "torch.mps.empty_cache()  \n",
    "# Load the Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base.en\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-finetuned-en\",\n",
    "    per_device_train_batch_size=8,\n",
    "    #To use cpu since gpu memory was exceeded - mac issues again\n",
    "    no_cuda=True,  \n",
    "    num_train_epochs=1,  \n",
    "    save_steps=500,  \n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=False, \n",
    "    gradient_accumulation_steps=4,  \n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator_n,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and processor\n",
    "model.save_pretrained(\"./whisper-finetuned-en/final-model\")\n",
    "processor.save_pretrained(\"./whisper-finetuned-en/final-model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "def split_audio(audio, chunk_duration=30):\n",
    "    \"\"\"Split audio into chunks of specified duration (in seconds).\"\"\"\n",
    "    sample_rate = 16000\n",
    "    chunk_size = chunk_duration * sample_rate\n",
    "    return [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "\n",
    "def full_audio_transcription(audio_file):\n",
    "    # Load the Whisper model and processor\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-finetuned-en/final-model\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"./whisper-finetuned-en/final-model\")\n",
    "\n",
    "    # Load and preprocess the audio file\n",
    "    audio, _ = librosa.load(audio_file, sr=16000)  # Load audio file at 16 kHz\n",
    "\n",
    "    # Split the audio into chunks of 30 seconds each\n",
    "    audio_chunks = split_audio(audio, chunk_duration=30)\n",
    "\n",
    "    full_transcript = []\n",
    "\n",
    "    for chunk in audio_chunks:\n",
    "        # Process the audio chunk to get inputs for the model\n",
    "        audio_input = processor(chunk, return_tensors=\"pt\", sampling_rate=16000)\n",
    "\n",
    "        # Generate subtitles (full transcript for this chunk)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**audio_input)\n",
    "\n",
    "        # Decode the generated ids to text\n",
    "        transcript_chunk = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        full_transcript.append(transcript_chunk)\n",
    "\n",
    "    # Join all chunks to form the full transcript\n",
    "    return \" \".join(full_transcript)\n",
    "\n",
    "def save_transcript_to_file(transcript, filename, folder_path):\n",
    "    \"\"\"Save the transcript to a file in the specified folder.\"\"\"\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Define the path for the transcript file\n",
    "    transcript_file = os.path.join(folder_path, f\"{filename}.txt\")\n",
    "    \n",
    "    # Save the transcript as a text file\n",
    "    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript)\n",
    "\n",
    "def process_all_audio_files(folder_path, output_folder):\n",
    "    \"\"\"Process all audio files in the specified folder and save transcripts as separate files.\"\"\"\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.wav', '.mp3', '.m4a')):  # Add other audio formats as needed\n",
    "            audio_file = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing {filename}...\")\n",
    "            transcript = full_audio_transcription(audio_file)\n",
    "            \n",
    "            # Save each transcript as a text file in the output folder\n",
    "            base_filename = os.path.splitext(filename)[0]  # Remove file extension\n",
    "            save_transcript_to_file(transcript, base_filename, output_folder)\n",
    "\n",
    "# Example usage\n",
    "audio_folder_path = \"audio_files\"\n",
    "output_folder_path = \"finetuned_en_whisper_sub\"\n",
    "process_all_audio_files(audio_folder_path, output_folder_path)\n",
    "\n",
    "print(f\"Transcripts saved to the folder '{output_folder_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
