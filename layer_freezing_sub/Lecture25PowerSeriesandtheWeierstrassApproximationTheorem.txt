 OK, so at the end of last time, let me just restate the theorem that we proved, which was the varish-strass.  M test, which I'll state in brief reform now, as the following.  things hold. First off, these m sub j's dominate the f sub j's. f sub j of x is less than or equal to m sub j. And two, these m sub j's are summable.  Then the conclusion is the sequence of partial sums. So converges uniformly. So there exists some function f from S to R, so that the partial sums converge uniformly. OK?  So this is a sequence of functions built up by the fj's by just taking the sum of the first j equals 1 to n. So for example, I kind of went through this last time, so example, if fj of x equals our old friend,  1 cosine of 160j x over 4 to the j, then, and let's say on R. So S is R. And a couple of things. fj of x for all x is less than or equal to 4 to the minus j,  because cosine of whatever you put into it is always bounded by 1. And since this is converges, this implies that this series, which I'm writing in this way, which you should think of as a limit  of functions of the partial sums, which are functions, converges uniformly on R. Not only that, this function defined by, I take x, I stick it into here, is continuous. Because last time, we proved that the uniform limit of continuous functions  continuous, this function here is the uniform limit of the partial sums, which is just, finally, many cosines of things, so that's continuous. So this gives another proof of one of the things we did by hand when we spoke about differentiability, namely that this function that we considered was continuous.  So we'll go back to power series, which was our original motivation. So I'm going to be saying that a series, so let me, again, make this perfectly  So we're clear. When I say, if you like, make this a definition, or really this is just, I was using this terminology up there in that theorem. And when I was talking about this example afterwards, but when I say a series involving functions converges uniformly, I mean the partial sums converge uniformly.  So means there exists a function f such that, and all of these are from S to R as well, such that,  OK, just so that there's no ambiguity. So if I have a sequence of functions, f sub j, and I form their series, and I say that converges uniformly, that means there exists some function, f, so that the sequence of partial sums, these are now functions, converges uniformly to the function f. And that's what power series are, right? They're expressions involving a free variable x, and therefore you  can think of them as a series involving functions. So let me just state the following theorem about when we have a uniform convergence. So let's suppose we have a power series.  with radius of convergence. Rho, which I will recall, is defined to be the limit as j goes to infinity, aj, 1 over aj, inverse. If this limit is 0, we say that the  of convergence is infinity. So this is a finite number. This is a meaningful expression. If this is 0, then this is shorthand for saying the radius of convergence is infinity. So let's assume this is. Then, as long as I stay strictly inside the radius of convergence,  I have uniform convergence. This is the statement of the theorem, then for all R and 0 or rho, this power series now thought of as a series of functions of x, converges.  So you know, you know, formally on x0 minus R, x0 plus R. OK? So the picture is, we have some radius of convergence, x0 plus rho, x0 minus rho.  So then we have uniform convergence of the power series. So we'll prove this using the Vierstrass M test. So let R be in 0 row.  We're going for all j. And OK, so the Vierstrass M test is for j equals starting from 1 and going to infinity. But you don't have to, just like for series, we don't have to start at 1. They can start at 0. So for all j, union 0, what do we have? What are we writing? And for all x,  this interval where we want to show uniform convergence, we have that this function a sub j x minus x not to the j in absolute value, what is this bounded by? This is bounded by absolute value of a j and then j and because x is in this interval, its distance to x not is less than or equal to R.  So R to the j. So this will be my Mj that I'll use for the Vierstrass M test. OK? Let me put that in parentheses, because I may not use that Mj. So these individual functions, just polynomials, are bounded by this number for each j.  And now I want to see if the series of involving these numbers converges. So let me apply the root test.  this is equal to, now this r to the j to the 1 over j just becomes r, and it can just come out of this limit, and I pick up a to the 1 over j, and this is equal to 1 over rho, right? So this is equal to, if you like, I can put r over rho, as long as if,  R is less than infinity, meaning if the radius of convergence is less than infinity and 0. OK? And what do we notice? Now R is less than rho, right? R is coming from the closed interval 0 up to the rho, not including rho. So this number, so this is always less than 1, but this number is always less than 1, too, as long as R is less than rho, which implies that the series,  from j equals 0 to infinity of aj, r to the j, converges. So now we have the sequence of functions, the polynomials that we use to build up our power series. Each of them bounded by this number, these numbers a sub j absolute value, times r to the j, and these numbers are sumable, the series converges.  So in test, this implies that the power series converges uniformly on this interval.  OK. All right, so as long as we stay within, strictly within inside the radius of convergence, we have uniform convergence of the power series. And therefore, using the theorems that we proved before, we can differentiate and integrate term by term the power series.  So which follows immediately from this previous theorem and then what we proved last time, which I'll state now.  So convergence, rho, positive. So let me write it this way. And the first is we're all see inside x0 minus r.  x not plus are, the function given by the power series, which I'm not going to call this f or anything. I'm just going to refer to the power series directly. This is differentiable at c. And to compute the derivative, you can just do it term by term, meaning I can take the derivative inside.  I d x of j equals 0 to infinity, a sub j evaluated at x equals c. This is equal to simply differentiating term by term.  OK? And number two, for all a less than b, well, with,  x naught minus rho, less than a, less than b, less than x naught, plus rho. So here I'm sticking to strictly with the inside the radius of convergence. There's the x naught plus rho x naught minus rho. And so now I'm going to integrate over some interval inside. Then I can integrate term by term. The integral from a to b,  sum from j equals 0 to infinity of a sub j, x minus x not j d x, equals, and I'll just write it this way, sum from j equals 0 to infinity, of, OK.  OK, and in fact, let me just to really emphasize that we're interchanging limits here, let me instead write this equivalently as sum from j equals 0 to infinity of j d by d x of a sub j x minus x not, j evaluated at x equals c.  So for a power series, I can interchange the limits. The limit being, taking a derivative and the sum, I can interchange. And then I can also interchange the integration and the sum, as long as I stay within the radius of convergence. And this statement here is a point-wise. OK, so let's see why one is the case. Two follows immediately from the sum.  what we've proven and so this previous theorem and the theorem we have about integration and uniform convergence. So what about one? So first off, we know we have uniform convergence of the series strictly inside of the radius of convergence. So how about we need to check to see if the,  The formal derivative also has radius of convergence as well, equal to Rho. So I claim that the radius convergence of the derivatives, which equals,  I can write it this way. So the derivative of this guy is j times a sub j times x minus x naught to the j minus 1. So just shifting indices, this is equal to,  So x minus x0 times j plus 1 times x minus x0 to the j. So our claim is that this power series here has a radius of convergence equal to rho, the original radius of convergence. So rho is the radius of convergence of, why did I come over here? I still had a board left.  The original power series has radius of convergence row. What we need to show to prove part one is that the radius of convergence of the derivatives of taking the derivatives inside also has radius of convergence row. OK? Because then this would imply that this power series, which is the derivative of that one converges uniformly on the same set. So then we have uniform convergence of the power series  and uniform convergence of the derivative of the power series. So by the theorem we proved in the last lecture, we would have that the derivative of the power series equals the power series of the derivatives. So I'm just going to focus on this claim. And this, again, just follows from what we know about limits.  we compute that if we take the limit as j goes to infinity, now these are the coefficients for the new power series, so aj plus 1, j plus 1, raised to the 1 over j, this is equal to limit, j goes to infinity of,  a sub j plus 1, now raised to the 1 over j plus 1, 1 over j plus 1, now I'll raise to j plus 1 over j. OK? Now, this was a special limit that we looked at last time.  OK? The limit as j goes to, not last time, but way back when we were looking at sequences, the limit as j goes to infinity of this guy is 1. So this, in fact, equals limit as j goes to infinity. This goes to 1. This goes to 1. So you can actually say this converges to 1 as well.  So to the j plus 1 over j, OK? Now, strictly speaking, you would have to, OK, so this thing also converges to what does it converge to. This, remember, is, so where do we have it? Do we still have it up here?  well, it's way over there. So let me recall that the radius of convergence, Rho, let me put an inverse here, this is equal to the limit as k goes to infinity of a sub k 1 over k. So the exponent's converging to 1. This is converging to Rho inverse again. So I get Rho inverse to the 1.  So therefore, this is supposed to be the radius, 1 over the radius of convergence to the differentiated power series, which implies the differentiated power series.  So j is 1 over this limit. 1 over that limit, which we computed is to be, is 1 over rho, which is the original radius of convergence. OK? So all this to say is that the radius of convergence of the differentiated power series, the formily differentiated power series, is the same as the radius of convergence of the original radius.  original power series. And therefore, you have uniform convergence of the derivatives, wherever you have uniform convergence of the original power series. And therefore, by the theorem we proved last time, since both the derivatives and so since the power series and the derivative of the power series converges uniformly on the same set, the power series  the infinices, so this guy is actually differentiable, and the derivative of the power series is the power series of the derivatives. OK? But you can iterate this, right? Because I now have this power series with radius of convergence equal to the radius of convergence of the original. And then I can take a derivative of that and show that's, as the same radius of convergence as the original.  So let me just leave this as a remark and not stated as a theorem. Iterating, you can prove that I want the kth derivative of the power series. So on,  So let me say. I'll x in here. The kth derivative of the power series is equal to the power series of differentiating term by term.  And so this holds for OK, equals 1, 2, and so on. And so in particular, if I evaluate at x0, this tells me that k factorial, a sub k, is equal to the derivative of this function, which you get out when you stick x into this power series.  And you can interpret this, although we never called them, Taylor series, as a statement that every power series is the Taylor series of a function. At least in this setting that we're looking at.  OK, so we've answered pretty definitively, at least for the scope of this class, when we can interchange limits, we can do that as long as we have uniform convergence of the objects that we're interested in, be it  Now, the function, continuous functions, or function and it's derivative, but there are more powerful statements out there that one can make, especially when it comes to integration. This is why, essentially, a different theory of integration was created, or one reason why.  but hopefully, if I see some of you in 18102, which is what I'm teaching next semester, we'll get into that further, and when we discuss Lebauge integration, that was thought of because somehow Riemann integration is not complete, the same way that the rational numbers are not complete.  And the LaBag integration is complete, in a certain sense, I'm being very vague here for a reason. So I'm just giving you where you can go with this. What the next step is, is at least improving when can you interchange two limits is really a topic that's fundamental to the study in the study.  of the La Baye integration. And you have much more powerful theorems there than you do here, which allows you to prove interesting results, and especially about Fourier series. In fact, if I had more time, we would apply, in fact, some of the stuff that we've done here, we could apply to the study of Fourier series, but maybe we'll do that in 18102.  I want to now prove the last theorem of the class, which is also due to the godfather. So when we had these power series, so these are defining functions, very special types of functions.  What are we called analytic? They are, by definition, essentially, the limit of polynomials. Right? They're the limit of these finite sums of a sub j's times x minus x 0 raised to the jth power. That's a polynomial. So for analytic functions, which are these functions equal to power series, they are the limits of polynomials.  else, but that's a pretty small class of functions, analytic functions. But the virus straws proved this very interesting result that actually something like this is true for all continuous functions. So roughly speaking, virus straws proved that,  Basically, every continuous function is, in some sense, almost a polynomial. Okay? Just as we saw for these analytic functions, meaning defined by power series, they are very close to being, at least within their radius of convergence, a polynomial. In fact, this is true for all continuous functions that not necessarily equal to a power series, but every continuous function is almost a polynomial.  And in what sense do I mean that? So this is Bierstrass's approximation theorem, which states the following. If f is in a continuous function, and on the unit interval say, you can make it A, B, just by,  at rescaling the variables, but I'll just state it for continuous functions on the unit interval. Then there exists a sequence, polynomials, a piece of n of x, such that,  So the piece of n converges to f, uniformly on 0, 1. OK? And so in this sense, every continuous function is well approximated by polynomials. So every continuous function is close to being a polynomial.  So I need to first prove a couple of things that will be needed. So first off, I'm only going to look at only, so let me make some remarks before we go to the proof. So we're putting the proof on hold now. So let me just make a remark.  for every f, just for certain f, and then for every f, we'll follow from this special class. So we only need to consider, we'll only consider the case, f of 0 equals 0, and f of 1 equals 0. So f is 0 at the endpoints.  so that we can extend f to a continuous function outside of the unit interval, by just setting it equal to 0. So let's suppose we prove in this special case, and we look at the general case.  What do we know that there exists sequence of polynomials, p sub n, such that? These polynomials, p sub n, converge uniformly to a small modification of f that results in,  So this function here, if I look at it, it's a continuous function on 0, 1, because I'm just modifying it by constants and then times x. And then at f of 1, I get f of 1 minus f of 0 minus f of 0, so I get 0. And at 0, I get f of 0 minus 0, f of 0 is 0, minus 0 times something, and then I'm going to say,  I don't care, equals 0. So there exists polynomials converging to this function now. If we've been able to prove the case, just for f of 0 equals 0 and f of 1 equals 0. So uniformly. And therefore, the polynomials given by a piece of n of x plus now x times f of 1.  minus f of 0 plus. So this is still, if this was a polynomial, so as adding this to it, that's still a polynomial converges to f tilde of x uniformly. And this, again, this is still a polynomial. So the whole point of this remark is that we only  we need to consider the case f of 0 equals 0 and f of 1 equals 0. And we're going to do that just so that we can extend the f to a continuous function outside the interval. All right?  converged to f is by what's called an approximation to the identity. Really, I guess you could think of it as an approximation to the delta function. And I'll explain that in just a minute. So for all n natural number, define c sub n. This will be the integral from minus 1 to 1 of 1 minus x squared, raised to the n  d x, 1 over, and then Q sub n of x, this is going to be equal to c sub n times 1 minus x squared raised to the n. Then a few simple consequences are. So first off, this is the integral of a function which is non-negative, but it's positive at plenty of places between  minus 1 and 1, and you proved in the homework that this means that the integral has to be positive. Then the first is we're all in natural number. We're all x into 0, 1. So the first two of these observations are very clear. Qn of x is greater than, so, minus 1 to 1.  2n of x is bigger than or equal to 0. So it's just c sub n, which is a positive number, times 1 minus x squared, x squared. So x is between minus 1 and 1, and therefore 1 minus x squared is always non-negative. 2 is also pretty clear. The integral from minus 1 to 1 of q sub n of x dx equals 1.  OK? Now, why is this clear? Because this is equal to the integral of 1 minus x squared raised to the n times c to the n, or c sub n. But c sub n is the inverse of that integral, so we should just pick up 1. Now, the third and less trivial thing, which is important, is that for all delta in 0,  1, this function Q sub n, I mean, this is just a polynomial, converges to 0, uniformly on the set, delta is less than, so x such that delta is bigger than or equal to the absolute value of x, no, delta is less than or equal to that.  The absolute value of x is less than or equal to 1. OK? So in other words, here's minus 1, 1, 0, delta, minus delta. If I look in these regions, so if you like the union of those two intervals, then Q sub n, this polynomial, is converging to 0 uniformly, as n goes to infinity on,  on the union of these two intervals. So what is the picture of what's going on? What do these Q sub n's look like? So here's minus 1. Here's 1. So kind of the first one looks like some constant times 1 minus x squared. So there's the first one.  and keeps getting bigger and bigger, this is 0 at higher and higher order at 1 and minus 1, and getting pretty small near here. And in fact, according to 3, if I take any interval around 0 and look outside of it, as n goes to infinity, Q sub n is going to 0 uniformly. So what it should look like is, maybe the next one looks like that, and then further still,  So like that, so that if I look inside over here or over here, Q sub n is going to 0 uniformly. So what you should think of is that the Q sub n's as n goes to infinity is something like a Dirac delta function, which is not exactly a function.  OK, so again, so let me re-emphasize that. And this is in quotes. You should think of Q sub n, x like delta function.  And x centered at x equals 0. OK? So that's in quotes, because that is meaningless. But some of you have taken physics, no? What properties are of a delta function? And the integral is 1. It's somehow 0 everywhere, away, except for the origin. And it's infinite there. But somehow, as integral 1.  So these Q sub n's will show form an approximation  So let's first estimate how big is this. So this constant c sub n, we don't really know what it is explicitly. But let's compute at least a rough size of it. And then we'll use this to prove the third part.  natural number, the following inequality, nx and minus 1, 1, 1 minus x square raised to the n, this is greater than or equal to 1 minus nx square. Now, if it's not, so it shouldn't be like, you know, hit you in the face clear why this is true, but one way you can prove it is that if you look at the function, g of x, x is less than or equal to 1 minus nx square.  x equals 1 minus x squared. And so first off, this inequality is even in x. Doesn't matter if x is negative or positive. So let's look at this function on 0 or 1. Then what's the point? If I look at g of 0, this is 0.  prime of x, this is equal to n times 2x times what? Times 1 minus 1 minus x squared to the n minus 1. And this is always less than or equal to 1, this thing in parentheses. So this thing is always bigger than or equal to 0 on 0 1. So on 0 1, this function is  increasing, and its value at 0 is 0, so we get that, which is exactly what we wanted to prove. So now we compute the size of c sub n. So to do that, let me look at 1 over c sub n.  If I want an upper bound on c sub n, I need to prove a lower bound on 1 over c sub n. So let's take 1 over c sub n and find a lower bound for it. This is the integral from minus 1 to 1 of 1 minus x squared raised to the n, d x. Now, I can't remember if I made this a homework problem or not, but for even functions, integrating over an interval even with respect to,  the origin, this is just twice, I mean, I'm sure you remember this from calculus, it's not hard to prove with what we know of the change of variables formula and so on, that this is equal to twice times the integral from 0 to 1 of 1 minus x squared raised to the n. OK, now this is bigger than or equal to 2 times if I integrate to a certain point, this certain point is chosen so that I get a result in the n that's,  So that's pretty, essentially. Now, this is where I replace this by the smaller thing, which is easy to integrate. So this is greater than or equal to 2, integral 0, 1 over root n, 1 minus n.  x squared dx, and I leave it to you to verify that, with this choice at the end point, what I get is for over 3 root n, 1 over root n. And therefore, so, and this is bigger than 1 over root n, so I say,  started with 1 over c sub n, and I showed that it was bigger than 1 over root n, and therefore, c sub n is less than 1 over, is c sub n is less than the square root of n. OK? Now we'll use this to compute what we want. In fact, I mean, we really just needed to show that c sub n is bounded by,  know some polynomial and n, but this will suffice. So we now want to show. So delta will be positive. We now want to show that Q sub n converges uniformly to 0 on that set where the absolute value of x is less than or equal to 1 is bigger than or equal to delta.  So we note that the following sequence converges to 0, that square root of n times 1 minus delta is squared raised to the n. So we should also put delta's n 0 of 1. So this sequence here,  converges to 0 as n goes to infinity. Now, intuitively, why is this? This is because this is some number less than 1, raised to the nth power. Exponential always beats just a polynomial or a power of n. If you want to see exactly why this is, we could compute the limit as n goes to infinity of this sequence.  raised to 1 over n power, then this is equal to the limit as n goes to infinity of n to the 1 over n, 1 1-half, 1 minus delta squared. And this converges to 1, we proved that. And so this equals 1 minus delta squared, which is less than 1. And we have this, you know, theorem from our statement.  section on sequences that says if this limit is less than 1, then the thing here converges to 0. Or you could interpret this as saying that the series with this as the individual terms converges, and therefore the individual terms have to converge to 0. So which implies limit as n goes to infinity.  equals 0. So now we have this, and you know, once we have this, we'll have what we want. So we want to prove uniform convergence on that set, so let epsilon be positive. Then since this sequence converges to 0, there exists an M, a natural number, so that for all n bigger than or equal to M,  So square root of n times 1 minus delta squared, raised to the n, is less than epsilon. And for all n bigger than or equal to m, for all x, so that delta is between less than 1, we get that. Q sub n, so it's non-negative, minus 0 in absolute value, that's just  use sub n of x, at least the n. This is less than or equal to square root of n. That's for this guy. 1 minus x squared. This is getting smaller as I get closer to 1. So it's biggest at delta. And how we chose M is this is less than epsilon.  OK, so now we're ready for the proof of the Vierstrass approximation theorem.  f of 0 equals 0, f of 1 equals 0. So this polynomial here is very concentrated at 0.  Well, first off, suppose f, and it's continuous here. So you can check that if 0 there, and I extend it to be 0 outside of 0, 1, this is still a continuous function. I'm just defining it this way, because I want to write certain symbols in a little bit without specifying exactly where the bounds of the integration are.  OK, so we extend f by 0 outside of 0, 1. And this function f is continuous function on the real number line. OK?  So we define, now we're going to define this polynomial, sequence of polynomials, p sub n. This is going to be equal to the integral from 0 to 1 of f of t times Q sub n of t minus x dt. And just to remind you, this is equal to the integral from 0 to 1 of f of t times c sub n, 1 minus x.  minus x minus t squared, raised to the n dt. OK, so this is just, you know, c sub n times this thing, if you expand everything out, is just a, you know, using the binomial theorem, this is equal to j equals,  I'm just going to put a sum here meaning a finite sum. Some numbers a sub j n times x to the j times t to the, well, just some finite numbers a jk times x to the j, t to the k. All right. All I'm saying is,  If you expand this out using the binomial theorem, you get this polynomial and x of j, t sub k. And then this is getting integrated against f of t dt. And so this is in fact a polynomial. And I'll give you a, you know, it. OK, so let's write this out and be precise, just so that.  you're convinced that it's a polynomial. This is equal to the integral from 0 to 1 of f of t times c sub n. Now we use the binomial theorem. This is equal to j 0 to n, n choose j, and then minus x minus t squared, raised to the,  c in minus j, or I could put j, 2j. And then dt, I'm not sure if this is even helping or anything, but just so that you see this is actually polynomial.  c sub n, sum from j equals 0 to n. And now, sum from k equals 0 to 2j, n choose j minus 1 to the j. Now, 2j choose k minus t to the k times x to the 2j minus k.  And then dt. So I have all this junk integrated dt, and then this is x to the 2j minus k. So that just pops out. So this is a polynomial. All right? I mean, this is a polynomial, but then when it integrates against f of t, this x to the 2j minus k comes out of the integral. And I get that times just all, you know, this becomes a sum of terms with x to the 2j minus k.  outside this integral times f of t times, you know, integrated against minus 2 to the k. All right, I think I said more than I needed to there, but so the point is p sub n of x is a polynomial. Now, we can write this slightly differently. So p sub n of x, this is equal to,  So this as it was before. Now this, we change variables and is equal to, so we do use substitution now where u is equal to x minus t. And then I'm going to change the,  dt, so what I did here was a change of variables. By setting u is equal to x minus t. I guess you could, if you like, t minus x. And then I just recalled, and then I just called ut, then again.  So I'm eventually looking at these polynomials only in the interval 0, 1. So OK, so this is what it looks like. But now, this f of x plus t for t between minus x and 1 minus x, it's actually 0 outside of that.  So I can extend the integration to minus 1, and 1, again, with the understanding that I've extended f to be 0 outside of 0, 1. So since f y equals 0, or let me write it this,  this way since f of x plus t equals 0 for t not n minus x and 1 minus x. OK? All right. So that was a lot of, you know, explaining for some stuff. But here, let me get to the point. What is p sub n, really? So why should you expect this to converge to f?  So this is a little discussion. So I said that, and this is for those who understood my comment about delta functions, you know, n Q sub n is very concentrated at 0, at t equals 0, OK? So this,  you should think of as being approximately like a delta function at T. If you don't know what a delta function is or never heard of it, then forget the rest of this remark. But then I'll say something. So Q sub n kind of looks like a delta function and therefore minus 1 of x plus T, Q n of T, dt, should,  to look more and more like f of x plus delta of t dt. And what we know about direct delta functions is that when you integrate them against the function, you just pick up the function evaluated at 0, which is OK. So that's kind of why you expect it.  this picture of what the Q-ins are looking like, they're concentrating more and more at 0. So all of the contribution to this integral here, which defines these polynomials, is happening at t equals 0. At t equals 0, I just pick up f of x. So that's why you should kind of expect these polynomials to converge to.  back to the function. OK? So the thing I can finish on this board. So this is a picture you should have seared in your mind. This is what the Q sub n's look like.  all of their mass right at the origin, and therefore, I should just pick up f at the point x. So since this is concentrating at the origin, t equals 0, I should just pick up f of x plus t at t equals 0, which is f of x. OK? And why f of x plus c, not some constant times f of x plus t, that's because of the integral of Q sub n's is 1. OK.  So now let's prove that the p-ends converge to f  since f is a continuous function on a closed and bounded interval, we know that it's uniformly continuous. And therefore, there exists delta positive such that, how am I writing this here?  So such that if, sorry, that this way, for all zy, so that z minus y less than delta, we get that f of z minus f of y is,  Actually, we don't need, OK, so I mean we don't need so much of about f, but we'll go with this anyways. We really just know, OK, and never mind, I'll stop. So we know that f is uniformly continuous, so there exists a delta, so that I have this. Yeah, so less than epsilon over 2.  So if z and y are within delta to each other, then f of z and f of y, no matter what z and y are, are within epsilon over 2 of each other. Now, since f is continuous, it has a maximum on this interval, well, I should say since f is a continuous function, it has both a max and a min on this interval, and therefore,  So there exists some number c such that f of x is bounded by c for all x and 0, 1. OK? All right. So now I have this delta coming from the uniform continuity of f.  coming from the fact that it's bounded on this interval. And now I'm going to choose my M for the uniform convergence of the polynomials, just depending on these pieces of input, then as we showed, since square root of n, 1 minus delta squared and convergence to 0, this implies there exists an M, natural number, so that for all,  n bigger than or equal to M, I get that square root of n, 1 minus delta squared, raised to the n, is less than epsilon over 8c. OK? All right, I claim this M works, that for all n bigger than or equal to M, for all x and 0, 1p and n,  n minus f is less than epsilon. And for all n bigger than or equal to m, for all x n 0 1, if we look at a piece of n of x minus f of x, we're going to use that this thing is an approximation to the identity, meaning it's, it's, you know,  essentially that it satisfies those three properties that I wrote a minute ago. So this is equal to integral from minus 1 to 1 of f of x plus t times Qn of t dt minus f of x. What's the integral from minus 1 to 1 of Qn sub t? That's equal to 1.  x is at times, so this is all of analysis, is writing 1 or 0 in a certain way. Not all, but. So I can write this as f of x plus t, now minus f of x times Qn of t. dt, again, because the integral of Q sub n of t equals 1. So again, if you just expand,  in this out, this is f of x, I'm integrating with respect to t, so I just pick up what I had before. Now, by the triangle inequality for integrals, this is less than or equal to the integral from minus 1 to 1 of f of x plus t minus f of x times the absolute value of Q sub n of t, but since Q sub n of t is non-negative, that's just Q sub n of t.  And I'm going to split this interval up into two parts. This is equal to a part where t is less than or equal to, so it should minus delta f of x plus t minus f of x, cuse of n of t dt. And then plus the other part, which I can write,  as, so it's a union of the two intervals now away from delta and out to 1. So it's the sum of the integral over these two intervals, which I'm going to write as the integral over delta less than or equal to t is less than or equal to 1.  times Qn of t. Now, t is between minus delta and delta, and therefore x plus t minus x, an absolute value is less than delta. So in this interval here, I want to note that if t is in this interval here,  here, x plus t minus x equals t is less than delta in this here. So since this guy minus this guy is less than delta, an absolute value, I can use this uniform continuity part to say that this is less than epsilon over 2 q n of t, dt.  plus now the absolute value of this guy is less than by the triangle inequality is less than or equal to the sum of the absolute values which is less than or equal to c to c so 1 and then 2c. OK, because that's bounded by c, that's bounded by c, so the absolute value of the difference is less than or equal to the sum of 1,  of the absolute values, which is bounded by, each of those bounded by c. OK? Now, this is less than epsilon over 2, integral from, if I just integrate the whole thing from minus 1 to 1, that just gives me 1, plus 2c times Qn of t on this interval is, again, so I should put a delta there.  2c sub n, 1 minus delta squared, raised to the n, dt, and so there's no t here, so this is equal to epsilon over 2, because this integral is 1, plus this,  So c sub n, remember, is less than the square root of n times 1 minus delta squared, n times the integral over this region dt, which I can make bigger by going from minus 1 to 1, it gives me 2, and this is equal to epsilon over 2 plus 4 c square root of n, 1 minus delta squared n, and this is less than epsilon over 2 plus epsilon over 2 equals epsilon. I think I have 1 minute to spare.  This was quite an experience teaching to an empty room. I hope you did get something out of this class. Unfortunately, I wasn't able to meet a lot of you, and that's one of the best parts about teaching and being able to see you grasp in real time what I'm talking about. So, hopefully this nightmare will end soon, and we'll get to see each other in the future. you're going to find a limit.