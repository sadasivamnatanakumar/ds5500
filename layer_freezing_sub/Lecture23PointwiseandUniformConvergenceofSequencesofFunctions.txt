 So last time, we proved the fundamental theorem of calculus, and as a consequence, integration by parts formula, and the change of variables formula, or use substitutions. So let me just recall for integration by parts.  is that if I had two continuously differentiable functions, then I can shift the burden of taking a derivative. f prime times g, in a great, is equal to,  f of b g of b minus f of a, g of a, minus the interval from a to b of g prime f. So I can shift that derivative over to from f to g. And this is, you know, apart from the triangle inequality, probably,  one of the most useful theorems that comes out of calculus other than the fundamental theorem of calculus. In fact, for those of you who've taken quantum mechanics, which, I mean, you don't have to, but, or you heard of something called the Heisenberg, uncertainty principle, which says something like something to the effect that you cannot measure the position of a particle,  And it's momentum to arbitrarily good degree. You're bound by, if you can measure the position of a particle very well, then your measurement of the momentum is going to be not so great and vice versa. And that's based on an inequality, and how do you prove that inequality, integration by parts. So integration by parts is, in fact, responsible for,  So one of the great head scratchers from quantum mechanics. So just to back up my claim a little bit, now I'm not going to give that as an application. I'm going to give a different application related to Fourier series.  And I'm not going to say what types. Suppose f from minus pi to pi, dar is 2 pi periodic. And so the question that arose due to Fourier in this study of heat transfer, so this is Fourier.  200 years ago. He made the following claim that the function f of x can be expanded in terms of simpler building blocks in terms of simpler.  functions. So we haven't talked about Taylor series. We will, just a minute, or power series, which you've come into contact with, which is a way of, if you like, expanding a function in terms of polynomials, and now, or monomials. Now, Fourier suggested that f of x can be  we expand it as a superposition of functions, which are too pi-periodic, and kind of the most basic two pi-periodic functions. Now, what is so special about sine x and cosine x? Well, this is a little bit deeper. The fact that they satisfy a certain second order differential equation, and they are all of the solutions to the second order differential equation that are too pi-periodic.  OK, so you should think of these as kind of being building blocks. Another way to think of this is analogous to, if you have a vector, so now this is not a partition, this is a vector.  as a sum of coefficients a sub n, well, a sub j, j equals 1 to n. You know what? Let me make this M. Let's make this M. Let's make this M. So it looks a little a sub n, a sub n, e sub n. We're now, a sub n. This is the basis vector.  given by 0, 1, 0, where this is in the spot. OK? So you can think of this expansion in terms of sines and cosines as being analogous to expanding a vector in terms of basis elements.  can think of it as a different way to expand a function other than power series. But these components arose in a natural way if one were to study the problem of a heat transfer, which is governed by the equation dtU equals dx squared U.  So a condition that u of 0x, so I time 0, equals f of x. OK? So OK. Now, just like for how we expand a vector, and to base this element, there's a formula for computing these coefficients. Right? OK.  they should be, a sub n should be x sub n over here, but what's a different way of obtaining these coefficients. So this is a vector in Rm. And so how do you obtain the coefficients?  since a sub n, well, if I take the inner product of both sides, the dot product, say, of e x dot, let's say, e sub n prime, let's say, OK, so now I've used m n, let's say L. This is equal to the sum from n equals 1 to m of a sub n, b sub n dot, e sub L. Now, I wrote these  basis vectors this way, because that's kind of a standard choice for Rm. And what makes them standard is they have unit length, and they're orthogonal to each other. So they form an orthonormal basis. So when I take the product of E sub n with E sub L, I pick up what is usually referred to as delta in L, where here delta,  nl, this is 1 if n equals l and 0 if n does not equal l. And therefore, this just reduces to a l. So we see that a sub l is equal to x.e sub l. OK? And this is, so all of this discussion was in the  setting of a finite dimensional vector in Rm, and expanding in terms of the standard basis here, but it didn't have to be. It could have been as long as it's an orthonormal basis, then I get this relation, that the coefficient that appears in front of that  So let's say we try and do this same thing, now with f of x, except now in state, so these are functions. So instead of taking dot products, which is a sum of, you know, components, let's take an integral. So, if I take,  f of x, and if you like, dot it with sine of x and sum, which is, you can think of as, you know, I said that the integral is, you should think of as maybe a continuous sum. What do we get, assuming that this expansion holds, this is equal to the sum from n equals 0, so let me make this L. This is,  The sum from n equals 0 to infinity of a sub n sine in x times sine. So let me, we're getting to write the intervals here. I'm skipping a point I want to make as well. Sum and just remember the sum is starting from 0 to infinity.  Lx plus bn cosine nx, n Lx, sine Lx, n equals 0 to infinity. OK, I'll just write it. Stop being lazy. Now, assuming I can do what I'm about to do, and that's actually going to be a lot of the motivation for what we're going to discuss.  to our final chapter, assuming I can take this infinite sum and interchange it with this interval. This is the interchanging of two limits. A sum is an infinite limit. An integration is a limit. So assuming I can switch these two limiting processes, then I pick up a sub n, a sub n,  minus pi pi, sine in x, sine in Lx, sine Lx, sorry, plus b sub n integral minus pi to pi, cosine in x, sine Lx dx. Now, you can actually sit down and compute this based on, you know,  trigonometric identities. And what you get is that this is always equal to 0. And that this here equals pi times delta nl. OK? So this equals a sum from n equals 0 to infinity, a n pi delta nl.  which equals pi times AL. OK? All right, so we get this quantity here is assuming everything we've done is kosher, equal to pi times a sub L. And then to pick up the b sub L is the same, except now you integrate, again, it's cosine of L x, so similarly,  So pi times B sub L is equal to the integral from minus pi to pi of f of x, cosine Lx dx. So the B sub L's and A sub L's are referred to as a 4a  coefficients of the function f. So if f from minus pi to pi to R is continuous,  And 2 pi-periodic, the numbers a sub n equals 1 over pi, and a roll from minus pi to pi, of f of x, sine in x, d x,  b sub n equals 1 over pi and I'll go from minus pi to pi. f of x cosine in x, d x, or refer to as the Fourier co-officiency of f.  And so just using integration by parts, so what's the first question one should ask? If it's even possible, or in what sense does f of x equal this infinite sum? Well, we haven't even gotten into that. But one question you can ask is, do these coefficients that come in front of these basic building blocks sign in,  the cosine nx, does that converge to 0? I mean, if we expect f of x to be equal to the sum of these basic parts, then the contributions from each should be getting smaller and smaller. So does a n and b n tend to 0 as n goes to infinity? And this is the content of what's usually referred to as the Riemann-Lebaic limma.  I'm going to state it this way right now, which is the following. If f from A B to R is continuously differentiable,  Then limit as n goes to infinity of a sub n equals the limit as n goes to infinity of b sub n equals 0. Now, the actual way the Riemann-Lobac-Limmerge limit is typically stated is, in fact, I don't need it to be continuously differentiable, I just need it to be continuous, this is still true.  but we haven't done or won't do in this class, approximation theorems for continuous functions, which says that if you can do this for continuously differentiable functions, and basically you can do it for continuous functions, but this will suffice. So what this says is that the contributions coming from these building blocks is getting smaller, at least in the sense that the coefficients are getting smaller.  about if that sum up there with the a and b ands to find this way, actually converges to f. I do want to emphasize that. In fact, trying to straighten out this question, in what sense this series converges to f is really the motivation for a lot of analysis. Developed past in the first part of the last  last century and the last part of the century before that, and forms the basis of what's called harmonic analysis, which is a really beautiful subject and still an active area of ongoing research. Okay. So how do we prove this? Well, I stated the integration by parts formula earlier.  in fact, it'll follow pretty easily from that. Let's prove that the limit as n goes to infinity of b sub n equals 0. The 1 for a sub n is similar. There's just the extra piece, but I'm going to be a little bit lazy and do the easier one. We'll show.  OK, so let's look at b sub n. This is equal to the integral from minus pi to pi of f of x. And in fact, let me write cosine in x times f of x dx. Why am I writing dx dx? And now what I do is,  do is cosine in x, I can write as the derivative of something, 1 over n times sine in x. If I take the derivative of that with respect to x, I get cosine in x. And actually prove that, but you can look back in your calculus textbooks. We've proven enough to be able to make that precise. So by integration by parts, I  can now shift the blame or shift the burden of this derivative onto f, but look what I've gained, I've gained a 1 over n here. So now this is equal to 1 over n sine n pi f of pi minus sine of n minus pi times f of pi minus pi.  s 1 over n, sine in x minus pi to pi, f prime of x, d x. And really what this computation is showing is, illustrating is the oscillatory nature of what's going on. Cosine of in x is oscillating as n gets very large, between minus 1 and 1, and equal footing.  on average, you're getting the same amount of positive f as minus, as negative f, so, or you're waiting f in such a way that it's both positive and negative and equal amounts. Now, sine of n pi, no matter what n is, I get 0, sine of n minus pi, I get 0, so this first part drops off, so this is equal to minus 1 over n, minus pi, sine in x,  x times f prime of x dx. And therefore, if I take the absolute value of b sub n, this is less than or equal to 1 over n minus pi by sine in x, f prime of x, dx, if I bring the absolute value inside, so I can bring the absolute value inside.  and still get this. So in fact, before, when I had the absolute value outside, it's an equality, but now it's a less than or equality. I mean, inequality. So now, sine of nx is always bounded by 1. So this is less than or equal to the integral of f prime of x dx. So this is equal to 1 over n, integral a, b, f.  Now, this is just f prime. This is just a fixed number times 1 over n. So this converges to 0 as n goes to infinity. And b sub n, an absolute value, of course, it's always bigger than or equal to 0. And it's bounded by something converging to 0 as n goes to infinity. So by the squeeze theorem, we conclude b sub n converges to 0.  And that's the proof. The proof for the a sub n's is similar, except now you can't throw away necessarily the endpoints, but it's still not a very big deal. OK. And in fact, if we have time, I'll show you how much.  Now, one can prove, and this is proven in classes on harmonic analysis, that, in fact, for a function which is continuously differentiable, this, and too, by periodic, this series actually does converge uniformly to f. On this interval, and I haven't even said what uniform convergence means, but actually does converge to the function f of x.  So this is the case for continuously differentiable functions. I'll give a proof later that, in fact, this series converges if f is twice continuously differentiable. We can actually do that using the fundamental integration by parts, again, essentially.  There are a few things here that are behind the scene that I kind of swept away. First off, when we computed these formulas, formulae, I guess, we interchanged infinite summation of functions with integration. When can we do that? In what sense does a Fourier series converges? What sense does it converge to f?  for convergence of real numbers, there was just one sense of the convergence of real numbers. Now, when we have a sequence of functions, which is now what we're going to turn to, we'll have different notions of convergence to another function. And depending on, in what sense that convergence takes place,  some of these limiting operations may not interchange. OK, so now we're going to move on to the final chapter of this class. And I know it seems like we're kind of hitting a lot of different things now towards the end of the class. And we took it slow during the first part of class, but that's, like I said, that's, I think I even said this at the start of class. We didn't have very much to go off of. We built things from the ground up in the more technology you have, the more  things you can prove, the more interesting questions you can ask. So now we're going to go onto sequences of functions. And you could also put sequences in series, because the series is just a special type of sequence of functions. So I motivated a little bit of why we would  would be interested in functions converging to other functions or sequences of functions converging to a function. But we could look at something much more basic. So let's look at take a step back and look at power series. And this should be thought of as motivation for what's to come, just like our discussion about Fourier series.  I'm not going to ask any deep questions about Fourier series on the homework or on the exam. So a lot of this is just, this discussion was to motivate this theorem here. But now I'm going to make kind of a more precise motivation, I guess, for what's to come. So although we've had series forever, I never brought up power.  series, and it's for a reason, and it's because I didn't think they belong anywhere, until we got to sequences of functions. So a power series about a point x not, is a series of the form sum from j equals 0 to infinity, of,  x sub j x minus x naught to the j. So the x naught is given. And the things that could change are the coefficients or this number x here. So theorem, which immediately follows from essentially the  root tests, suppose this number R, which is the limit as j goes to infinity of a sub j 1 over j exists. So it's a finite number, positive non-negative number, and define rho to be,  1 over R, if R is bigger than 0, and infinity, if R equals 0. Then we have the following conclusion that this power series, a sub j, converges.  absolutely if x minus x0 is less than rho and diverges if x minus x0 and absolute value is bigger than rho. And this number rho we refer to as,  a radius of convergence. So, again, the proof follows immediately from the root test, because if we take limit as x  this j goes to infinity of a sub j, absolute value, x minus x not, j, 1 over j, this is equal to x minus x not, this kills that j, this is just a fixed number, so this pops out of the limit, and this limit exists, so this equals x minus x not,  R, and we have two things happening. This is less than 1 if x minus x0 is less than rho, bigger than 1 if x minus x0 is bigger than rho. This number here. And therefore, by the root test,  So we see that this series, if you like, what's given are the coefficients a sub j and x naught, and what's could change is x.  that this series converges as long as x minus x0 is less than rho. So as long as we stay in that interval, that's the metric interval about x0, then this series converges.  absolutely. So we can define a function where if I take x in this interval, let's take it into this series, I get out a number. So define function f, now going from this interval, so x0 minus rho, x0 plus r, to R, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1, x1  by f of x equals the number that gets spat out by this power series. So I'm going to put the j there. So for example, what is f of x?  So let's say I take all of the coefficients to be, let's say x not as 0, and all the coefficients are 1. So let's say I look at some of x sub j, then f of x. So we've already computed for a geometric series. This is equal to 1 over 1 minus x for x and minus 1 1. OK?  minus x is equal to x to the j. Another example is that if I take a sub j equals 1 over j factorial, x not equals 0, then you've done this in an exercise that this series here,  converges absolutely for all x, meaning rho equals infinity, and this is how we define the exponential function.  exponential function, exponential of x, to be what comes out of this series. And then simply from this definition, you can show things that an exponential function should satisfy. x to the n is equal to n, e to the e of 1.  to the n's power and so on and so on, so that this really does obey what you believe an exponential function should look like. And also it grows faster than any power of x as x goes to infinity, it goes to 0 of those types of things. But this is how the exponential function is defined. So, OK, we have this function that's defined by whatever this power series spits out for x and the limit.  inside this interval of convergence. So then I could write f of x as the limit, as n goes to infinity, of a sequence of functions, because this is just how it's defined, where f n of x is the partial sum.  OK, it's just a polynomial, right? So for a power series, the limiting function, you can write it as the limit of the partial sums, which are just polynomials. OK? And so we should say for all x in this interval,  We have this. So now, some questions arise. So what? This function is equal to the limit of these, maybe simpler functions. These simpler functions are just polynomials for the case of power series. So, like I said, 1 over 1 minus x is equal to the limit of these polynomials. Some questions should arise. I mean, analysis is about limits.  you can think of that as half the story. First off, what is the limit? What are the important limiting processes that we consider? The second question is, how do different limits interact? So let's pose that as a question now for a three-part question for a power series. And this will motivate  And this is, again, motivating all of what we're going to be doing now. So is the function that I get as this limit of polynomials as the output from a power series? Is it continuous? The individual pieces that I take a limit to get f of x, these polynomials,  the partial sums are certainly continuous, they're just polynomials. So is the limiting thing continuous? Now, if so, is f differentiable? And in particular, since f is equal to the limit as n goes to infinity of the f n's, this f prime equal to the limit of the f n's.  limit as n goes to infinity of the f in prime. OK? So the derivative is a limiting process. So I'm taking the derivative of the limit. So I'm asking, can I take that derivative inside the limit? Can I swap the two processes? And the same with integration. If 1 does,  So the integral of f equal limit as n goes to infinity of the integrals of the f n's. So again, this is a limiting process that we're asking us to flip, because f is equal to the limit as n goes to infinity of f n, and what I'm asking is, can I take this interval?  will enroll inside that limit. Now, you can ask these questions not just for power series, but in a more general setting, which is what we're going to turn to now. But this should be in the back of your mind as the motivation for what we're doing. And apart from being just an academic question,  It, apart from being an academic question, it's also somehow giving you information over whether the formal manipulations that you're doing with Fourier series that are actually somehow modeling some physical phenomenon are these formal manipulations even meaningful. So these are the three questions that motivate what we're going to do going forward.  We don't have to just stick to the setting of being in power series. This should be a kind of very important example of a sequence of functions converging to a function, a limiting function. And then we can ask these questions. But we don't have to just stick to power series. So let me move on to a more general setting in which we'll answer these three questions.  And two modes of convergence for limits of functions. They're sequences of functions. So first definition, this is, in fact, what we showed, or what we were talking about before,  So in natural number, but f n be function from S to R, S is some, non-empty subset of the real numbers, and let f be from S to R. We say, f n, so the sequence of functions  fn converges point-wise to the function f, if for all x and S, by sticking x into S, so for each fixed x and S, if I stick this into f in of x, I get a limit, and this limit is f of x.  So for example, going back to power series, if we defined f of x, so let me just rewrite that example that I had up there. If I define f of x equals 1 over 1 minus x, f in of x to be some from j equals 0 to n of x j, then, in fact,  for all x and minus 1 to 1, limit as n goes to infinity of f n of x, equals f of x, i.e. this, the sequence of partial sums of corresponding to this power series converges to 1 over 1 minus x point-wise, OK, on minus 1 1.  So I say whenever you come across a definition, you should negate it. But the negation of this definition is not too difficult. A sequence of functions is not converged to another function point-wise if there exists some point. So that, when I stick them into fn of x, f n of x does not converge to f of x.  of x. So let's look at another example, which is not a power series. Let's say we take f in of x to be x to the n, where x is in the closed interval 0, 1. So what's happening here as n gets very large.  1. There's, I don't know, f 5 of x. And then as n gets very large, these guys are dropping down even more. And more. And what is, are we picking up something in the limit? Well, let's look. Well, if x equals 1, it's pretty clear that the limit as n  goes to infinity of fn of 1, this equals 1. If I stick in 1 here, I get 1 for all n. And therefore, the limit as n goes to infinity of f n of 1 is 1. Now, if x is in 0, 1, then we've done this limit before, limit as n goes to infinity of f n of x, this is equal to limit as n goes to infinity of x to the n.  less than 1, so x is being raised to higher and higher power, this equals 0. Thus, what do we conclude? For all x and 0, 1, this sequence of functions, x to the n, converges point-wise to the function x.  f of x, which is equal to 0 if x is in 0 1 and 1, if x equals 1. So I draw another picture here of what the limit looks like.  And you can kind of start to see this. As n gets large, again, this is becoming more vertical there, but then going to 0. So for any fixed n, it's converging to this picture on the right. And so we can already pick up something, or at least answer, one of these questions, if we take them as,  question about convergence or functions in the point y sense, so we could have asked this question now right after having this definition. Suppose f ins are continuous converging point y's to a function f. Is the function f continuous? And what this example shows is that, no, that's not the case, right? x to the n is always continuous, yet the limit as n goes to infinity, the point y's limit is given by this function, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is 0, which is, which is 0, which is, which is, which is, and, which is,  0 from 0 to 1, and 1 at x equals 1, which is not a continuous function. So already, we're kind of saying that point-wise convergence, which is this first weakest mode of convergence, that we, as of now, can say about power series, is not good enough to ensure that the limit is even continuous, because this example shows that, in fact,  shows that we have a sequence of continuous functions, whose point-wise limit is not continuous. So as another example, you should always like this kind of last chapter, because you can draw a lot of pictures.  So I'm going to draw pictures of fn. So it's piecewise linear. OK? So now I can write down what the function is, but I don't want to. I'm just going to draw pictures. So f n of x from 0, 1 to R, this is how it looks. So there's 1, 1. And what I do is I go to the point 1 over n.  the function fn of x is 0 up until then. And then it's a linear function connecting 1 over n to 2n here. And then it connects this point 2n 0, so, or I should say 1 over 2n, 2n connects that to the origin. OK, so that's f sub n of x.  is linear. So for example, if I want to draw f 1, f 1 would look like there's 1 0, 2, 1 half. And let's say I wanted to draw f 100. What does that look like? So maybe I should make this one a little bit bigger.  is 1, 1 over 100, and then that should be 1 over 200. And then if I go up to 200, this is a piece-wise linear function, which is,  it's getting, it's 0 from 1 up to 1 over 100. So it's 0 most of the time, and it's 0 at the origin. But in this in between, it's very tall and very slim. OK? And so my claim is that for all x and 0,  0 1, limit as n goes to infinity of f n of x, equals 0. OK, so this sequence of functions converges point-wise to 0. OK? All right, so why is this? Well, let's just give a whole proof of this.  rather than me talking it out. I mean, I'll talk it out and give a full proof. OK, so let's look at the easiest spot first. And I don't even need the formula for these guys. I just need to know that they have this basic characteristic that their point-wise linear, I mean that their piece-wise linear, connecting 0 to,  1 over 2n, 2n here, and then down to 1 over n 0, and then there's 0 between that and 1. So first off, if x equals 0, then all these functions are 0 at the origin. So they equals 0, so that equals 0.  So that's fine. So now suppose x is in 0, 1. OK? And so what do we want to show? We want to show limit as n goes to infinity of f n of x equals 0.  And here, so what's the point here? Now, there's 1, there's x. So I have to give a, well, I'm not even going to do an epsilon delta, epsilon m argument. I'm just going to show you what happens. So there's x between 0 and 1.  Now, let's choose a very large integer. So that 1 over m is less than x. So here's 1 over m, strictly to the left of x. Now, what does the graph of f in of x look like for n bigger than or equal to m?  x equals 1 to x equal 1 over M, and then, you know, shoots up, and then comes back down over here to 0. But here's the point. It's 0 all the way from 1 to 1 over M. So in particular, at x, f n of x is 0. OK? So if I look at this sequence, f n of x, which I'm trying to show converges to 0, it is 0.  So no, it is f 1 of x, f 2 of x, f n minus 1 of x, and then at f m of x, so at the now, so this is f m of x spot, it's 0. And this point is only going to the left. So for all n bigger than or equal to m,  Now, this will be 1 over n will be to the left of x, and therefore, f in of x will be 0. So this is 0, 0, 0, 0, and so on. OK? So I have the sequence is eventually 0 for all n bigger than or equal to capital M.  So and therefore, the limit, I mean it's pretty easy to take a limit of a constant sequence, and which proves that this sequence of functions converges point-wise to 0. Now, I didn't come up with this fancy example for just any old reason. It'll come back in a minute.  when we start answering some of these questions, or asking them. Again, within the context of these two convergence, you know, ways of converging. So far, I've only given you one definition of convergence, point-wise convergence of a sequence of functions. And now I'm going to give a slightly, I'm going to give a stronger, it's not slightly, it's much stronger,  definition of convergence of a sequence of functions. So we have a sequence of functions, and a given function from S to R. S is a non-empty subset of R. Then you say,  The sequence, f in converges point-wise, or uniformly to 0.  So if now we have an epsilon n statement, for all epsilon positive, there exists an M natural number such that we're all in bigger than or equal to M, for all x and S, f n of x minus f of x is less than epsilon.  Now, I want to make a brief comment. This looks suspiciously like point-wise convergence. If you just wrote down what it means for the limit as n goes to infinity of f in of x goes to f of x, except there's a very subtle and important point. And that is where does the for all x and S appear. For point-wise convergence, you can state point-wise convergence as this,  being at the start of the line for all x and s, for all epsilon positive, blah, blah, blah.  converges to f of x. For each x, I get a sequence of numbers, which converges to f of x. Now, uniform convergence is actually saying something stronger. And I'll say that. So let me, in fact, let me draw a picture that goes with this definition. Let's make S to be an interval.  So let's say my limiting function is f is given by this graph. And so what I'm going to do is basically shift the graph up and down by a limit.  epsilon, meaning this length is epsilon, and so is this linked, all the way across. So let me shade this in and re-outline, OK? So I get this little, if you like,  is a shaded area around my function f. So this is f of x. That's the graph. And the shaded part, this is the set of all x and y, such that f of x minus y is less than epsilon. OK, so I get a little tube, snaking with f. All right?  that for all n bigger than or equal to some M, so given epsilon, for all n bigger than or equal to M, if I were to draw the graph of f in of x, it better fall inside this tube across all of a b. See, this tube is defined for all x between a, b. So it's making a statement about how close f in,  of x is to f of x, across the entire set. Pointwise convergence just says, if I put an x into f n of x, then eventually that number is, those numbers are getting close to f of x. Uniform convergence is a global property. It's saying, across the entire set, as n is getting large, the graph of this f n is getting very close to the graph of f of x.  fixed in x, the fn of x at that point are converging to f of x. So I've said a couple of times that uniform convergence is stronger than point-wise convergence. Let me actually prove this now. So let me prove that following theorem.  So if I have a sequence of functions from S to R, and f in, rather than write, converges to f point-wise, or uniformly, I'm going to put an arrow, and then with the description afterwards, uniformly on S.  then converges to f point-wise on S. So it's very simple. Again, what is the picture that's going on for uniform convergence, is that f n is getting close to f across the entire set that we're looking at. So certainly at one point, which is all you need for point-wise convergence. Each fixed point, we should be getting close.  So let epsilon be positive. So first off, let's fix a number in the set S. So now we want to prove that the limit as n goes to infinity of f in of c equals f of c. Let epsilon be positive.  since f in converges to f uniformly, there exists a natural number M0 such that for all n bigger than or equal to M0, for all x in S, f n of x minus f of x is less than epsilon. So choose,  M to be this M0, the M that is for this epsilon. Then we're all in bigger than or equal to M. Let's call this equation star. Star with, at the single point x equals at c, implies f n of c minus f of c is less than epsilon.  And thus, the limit as n goes to infinity of f n of c equals f of c. So I don't think I have enough time to do the example that I want to do. So I'm just going to leave you on the edge of your seat by stating the following theorem.  term that in fact, so this is a one way street, meaning point-wise convergence does not imply uniform convergence. So we just proved uniform implies point-wise, but the converse does not hold. And what we'll prove next time is for in the setting of this simple example of x to the n. So.  And so what we'll prove next time is the following. If I take any b between 0 and 1,  then fn converges to f uniformly on the set 0b. So these functions are defined on 0 1, so they're certainly defined on 0b for b less than 1.  functions does not converge uniformly to f on 0, 1. So here, this second part, since the f n's converge to f point-wise on 0, 1, the second part says that this is a one-way street. This is not a two-way street.  modes of convergence, uniform convergence, and point-wise, are not equivalent.