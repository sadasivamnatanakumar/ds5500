 last lecture we introduced the notion of the limit of a function as x goes to c which we write limit x l or c f of x equals l what does this mean this means for all epsilon positive there exist a delta positive such that for all x and s satisfying 0,  was less than x minus c is less than delta, we have that f of x minus L is less than epsilon. And we proved the following theorem last time that if we have a set, a cluster point of s, so this is where we look at limits, and f is a function from s to R, then the limit as x goes to c of f of x equals L, if and only if for every sequence, x is less than epsilon,  x sub n converging to c, we have f of xn, the new sequence converges to L. So this theorem here connects limits of functions to what we did previously, limits of sequences. Now, using this theorem, we'll get analogs of theorems we proved for sequences, but now for limits of functions.  limits. First, let me show a few simple applications of this theorem. So for example, we could prove the following that we're all C and R, limit as x goes to c of x squared equals c squared. So we could have done this  using the just the definition, but now with this theorem, we can prove it a little bit quicker and easier. So, because we essentially did all the hard work when we proved that the product of two convergent sequences is convergent, so, which is what we'll use here. So, we're going to prove this theorem using the previous theorem. So, let xn,  So for this theorem, since it's not stated, you should take the function to be f of x equals x squared, and then the set that is defined on S is equal to R. So let xn be a sequence such that xn converges to c as n goes to infinity. We now want to show that f of xn converges to f of c, where f of x equals xn.  x squared, but this follows from what we proved for limits, right? So we proved the product of two convergent sequences is convergent by earlier theorem about the product of convergent sequences.  sequences, we get that xn squared converges to c squared, which implies. So we've now verified for every sequence converging to c, f of x in, meaning xn squared converges to f of converges to c squared, and therefore by the previous theorem,  We've proven the claim. So now let's use the theorem to study a couple of more limits. And we'll use this theorem, in fact, to show that a certain limit does not exist.  The limit as x goes to 0, I've signed 1 over x. This limit does not exist. So remember, for a limit, I look at all those points x that are close to c, but not equal to c. So this function, which I'm taking the limit as x goes to 0 of,  doesn't need to be defined at x equals 0 in order to consider the limit. And if you like, the function sine of 1 over x is defined on s equals R take away 0. So this limit does not exist, but the limit as x goes to 0 of x times sine of 1 over x does exist and equals 0.  OK? Now, I want you to note something. You can't just stick in x equals 0 here, all right, to evaluate the limit. I mean, because then you'll be taking sine of 1 over 0 times 0, you can't divide by 0. So it's this, you can't just say this limit equals 0, because I stick in x equals 0. In fact, we saw in the previous lecture that the limit need not equal the function evaluated at that point.  So just to recall the example from last time, when I had f of x equals 1 if x equals 0 and 2 if x does not equal 0, then we showed that limit as x goes to 0 of f of x equals 2, which note does not equal f of 0. OK, when we discuss continuity, that's what connects x to 0.  limits the limit to the function evaluated at that point. I just want to make that little comment. OK, to prove this theorem, we're going to use, again, the previous theorem. Maybe I should, now I'm not going to label it. You'll know when I point up to it that I'm referring to that theorem that I stated up there. So in fact, let's prove the second theorem.  limit exist and equals 0 first. So suppose xn is a sequence converging to 0. We now want to show that x sub n times sine of 1 over x sub n converges to 0. And then that, by the previous theorem up there, well,  imply that the limit as x goes to 0 of x times sine of 1 over x equals 0. OK? Now, if I look at the absolute value of x of n times sine of 1 over x of n, this is equal to the absolute value of x of n times the absolute value of sine of 1 over x sub n, and no matter what you stick into sine,  sine is always bounded between 1 and minus 1. So the absolute value is always bounded by 1. So just to summarize, we've shown that. OK? And now we apply the squeeze theorem. So you like that goes to 0.  since x sub n is converging to 0, remember we're assuming that, this converges to 0, and therefore what gets trapped in between goes to 0. So by squeeze theorem, x sub n times sine of 1 over x sub n converges to 0, and that proves the second claim.  OK, so now we'll prove 1. And in the previous lecture, I did negate this definition. So let's actually negate this theorem if you like, or use the negation.  negation of each side of this if and only if, to state an equivalent theorem. So two statements are equivalent, which is in that theorem, if and only if their negations are also equivalent. So by the theorem, we also have the following fact. Limit x goes to c of f of x is not equal L.  if there exists, so negating the right-hand side of that, if and only if there exists a sequence xn converging to c, so this sequence will, consisting of elements of s take away c, such that xn,  converges to c, but we don't have f of x n converging to L. And OK, and when I write this, you should read this as either this limit exists, and does not equal L or this limit does not exist. OK?  and write that out. So and either limit does not exist or does not equal L. So again, that's an equivalent way of stating the theorem.  Up there is in terms of the negation. Two statements are equivalent, which is right there, if and only if their negations are equivalent.  that there exists a sequence converging to 0, such that when I plug that in to sign, the limit of that sequence does not exist. So to show there exists a sequence converging to 0, such that,  Limit as n goes to infinity of sine of 1 over x. So then, does not exist. OK? Now, sine oscillates between 1 and minus 1, depending on if I'm a certain multiple of pi over 2. Right?  there's a lot of intuition behind it, behind the sequence I'm about to give you. Note that sign of x equals 1 if x equals pi over 2, 5 pi over 2, 9 pi over 2, and so on.  minus 1 if x equals 3 pi over 2, 7 pi over 2, 11 pi over 2, and so on. So I can stick in things that are getting bigger to sign and get 1 or minus 1. And in fact, let me change this to y, since we're using x.  to be essentially 1 over y. So if I stick in pi over 2, 5 pi over 2, 9 pi over 2, and so on, I get sine equals 1. If it's 3 pi over 2, 7 pi over 2, 11 pi over 2, and I stick that into sine, I get minus 1. But that means if I stick 1 over these numbers into sine of 1 over x, I get 1 or minus 1.  will be 1 or minus 1 alternating. And we know that sequence does not converge. So that's the idea. So let me write that down. Let x sub n be 1 over these numbers, essentially. So 2n minus 1 pi over 2 minus 1.  1, because we're going to stick this into sine of 1 over x. So let me just, which is 2 over 2 and minus 1 over pi. OK. Now, note, we're all in x of n is less than or equal to, I can write  like this as 2 over n plus n minus 1 pi, and since n is bigger than or equal to 1, this is always bigger than or equal to 0. So this is less than or equal to 2 over n pi, and this goes to 0. So that shows by the squeeze theorem that this sequence I've defined here converges to 0.  over x, this is now equal to sign of 2n minus 1 pi over 2. OK? And this is therefore equal to 1 minus 1, 1 minus 1, 1 minus 1. And this sequence, which is just equal to minus 1 to the n plus 1, y capitalized that does not converge. So we found a sequence converging to 0, such that when I stick it,  take it into the function that new sequence does not converge. So we proven that this limit does not exist.  All right, so I alluded to this fact that this theorem will give us, uh, um, theorems that are similar to what we prove for sequences, except now for, um, limits of functions. So let me just state, uh, kind of the simplest theorem you can get. So,  let S be a subset of R, see a cluster point of S, and suppose I have two functions, f going from S to R, and g going from S to R.  if these two limits exist, and one function is smaller than the other,  Then, so we had an analogous statement for sequences, which was if I have two sequences converging and one is less than or equal to the other, then the limits are, one limit is less than or equal to the other. And it's an analogous conclusion for limits of functions.  So again, analogous statement for sequences was we have two sequences, one less than or equal to the other, which then the limit of the smaller sequence is less than or equal to the limit of the bigger sequence. OK? So let's give the proof. And we'll use this theorem, connecting limits.  limits of functions to limits of sequences, and then we'll use the corresponding statement, which we do have for sequences, which I just stated a couple of times. So let L1 be the limit as x goes to c of f of x and L2 be limit as x goes to c of g of x. And what we want to show is that L1 is less than or equal to L2.  to, right? OK. Let xn be a sequence in S take away c such that xn converges to c. Such a sequence exists, because  as c is a cluster point of s. And you proved in the assignment that if I have a cluster point of s, then there exists a sequence in S take away c that converges to c. Now, since I stated it this way, by the previous theorem, this limit equals L1, this limit equals L2, if and only if for every sequence converging to c,  f of xn converges to L1, g of xn converges to L2. By the previous theorem, we then conclude that L1 is equal to the limit as n goes to infinity. Actually, I'm getting ahead of myself a little bit. So let's pause right there and reset. So now we have the sequence converging to c.  then by the assumption here for all n, f of xn is less than or equal to g of xn. And since f of xn converges to L1 and f of xn converges, g of xn converges to L2,  We get by, again, this theorem about sequences, which says if I have two sequences, f of x sub n, this is one sequence, is less than or equal to another sequence, g of x n. And they both converge,  OK, which is what we wanted to prove.  I said this theorem here follows from the analogous, in fact, I should have written this out. Or in fact, I will write it out now. So the analogous statement for sequences was if for all n a n is less than or equal to b n, then limit as n goes to infinity, assuming both of the sequences,  So limits exist satisfy the same inequality. So this is analogous theorem to this theorem which we had for sequences. And we use this theorem from sequences to prove it. Now, following that same philosophy, you can prove analogous statements for functions, limits of functions, as you did from sequences. You get these for free.  of the theorems we did for sequences, except now for limits of functions, I'm just going to kind of quickly say, you know, you get the same thing. So by using the previous theorem, which connects sequences of, which connects convergence of functions, to the right,  to convergence of sequences, we have analogous theorems for. And let me state it this way.  or limits of functions now. And for example, you have a squeeze theorem. Namely, if I have, so just talking this out, if I have three functions, say f is less than or equal to g is less than or equal to h, and f of x and this converges to L, h of x,  x converges to L, then g of x converges to L. OK? That's what I mean by an analogous statement. You also have theorems about algebraic operations and limits, meaning that if I have two functions,  that have limits as x goes to c, then f plus g will have a limit as x goes to c in that the limit of the sum is the sum of the limits. Same thing with the product and same thing with the quotient assuming the limit on the bottom is nonzero. And then similarly we also have,  I have a theorem about the absolute value and limits, namely if f of x converges to L as x goes to c, then the absolute value of f of x converges to the absolute value of L as x goes to c. So you have all of these analogous statements or theorems that are analogous to the statements from what we did for sequences, but now for limits of functions as x goes to c. And I'm not going to state them all.  So you can see this in the textbook. Maybe the proof of some of them I'll give as exercises. Now, let's separate the notion of a limit of a function from that of a sequence just a little bit. So unlike when we talk about limits of sequences, here we're letting a point x converges.  which or get close to a point c, but there's two ways it can get close to c on the real number line. It can converge to c from the left or it can converge to c from the right. And this leads to the notion of left and right limits of a function, left and right limits of,  functions. So let's start the definition here, and we'll go to the next board. So let s be a subset of R, and suppose c is,  is a cluster point of minus infinity 0 intersect, or c, intersect S. So what I'm defining now is the notion of a function converging to something as x goes to c from the left. OK, that's why I'm looking at only S intersect minus infinity to c. So I'm only looking to the left of c.  We say f of x converges to L as x converges to c from the left by putting a minus sign up here. If, and to some where, definition as that of a theorem, or that of the limit,  getting close to c from the left. If for all epsilon positive, there exists a delta positive such that for all x and s satisfying c minus delta is less than x is less than c. So it's close to c, but to the left of c, we have that f of x minus L.  x goes to c, where the minus sign up top, f of x, equals L.  converging to or taking a limit of a function as x goes to c from the right. And c is cluster point of c infinity, c common finity intersects. So I should say if. Suppose c is a cluster point of,  So now, just taking S, that's to the right of c, then we say that f of x converges to L as x converges to c plus meaning as x converges to c from the right. If we're all epsilon positive, there exists a delta positive, such that for all x,  all xn as satisfying, c is less than x, is less than c plus delta. So now it's close to c, but to the right of c, we have that f of x minus L is less than epsilon. And similarly to the notation up there, we write limit as x goes to c plus,  of f of x equals L. OK? Now, you know, just like we proved this theorem for limits, you can state, and prove an analogous statement for one-sided limits. So such a statement would be, for example, limit as x goes to c minus of f of x equals L, if and only if for every c,  The sequence x sub n satisfying x sub n is less than c, converging to c. We have f of x n converges to L. So these two just kind of limit how f behaves near a point c if we're just looking to the left of c or to the right of c. OK? But not at c. So let's, for example, look at,  I think this is usually referred to as the heavy side function. So f of x equals 0, or x less than or equal to 0, and 1, if x is less bigger than or equal to 0. OK, so graph is like that. And why do people care about this function?  certain sense if you take the derivative of this function you get what's called the direct delta function although that's not a function that's a distribution but you know that's why this function has a name to attach to it because if you take its derivative you get something somewhat special. Anyways we're not even at derivatives we're not even going to talk about distributions in this class so let's get back to one sided limits.  if I look at this function for x close to 0 from the left, f is just 0, so. So in fact, I can, since I'm only looking at x to the left of 0 in this limit, this is just plugging in x less than 0, so I get 0,  0, OK? And if I look at this function from the right, to the right of 0, then f of x is just 1, it's just 1 identically, and so I get, and it's again, so although I haven't shown that one-sided limits of constants equal the constant, I think that should be something  you can easily believe or write out yourself. So for this function, we see that it does have two-sided limits, except those limits don't equal each other. And they certainly don't equal, so this one does equal f of 0, but I could have made f of 0 to be a 1-half, and then this still would have been 0 and not equal to the function evaluated at the point. So again, I'm making this  point that for limits, OK, just limits, it does not matter what the function is doing at the point, all right? Limit only cares about how a function behaves near a point, one-sided limits, augment that by saying we're only going to care about the function near the point and to the left limit and to the right for the right limit. So I didn't say that, but this we call the left limit, this we call the right limit.  simply because we're coming, getting close to c from the right and from the left. OK? All right, so what is the connection between left and right limits we have the following.  let S be a subset of R, F be a function from S to R, and suppose that c is cluster point of both sets minus infinity to c intersect S, and c infinity intersect S. OK, that way I can talk about the left and right limits of c.  or at c. Then, so first off, if c is a cluster point of any one of these sets, it's going to be a cluster point of the set S. So we can actually look at the limit. So then the limit as x goes to c of f of x exists and equals L if and only if the limit as x goes to c of f of x exists and equals L, if and only if the limit as x goes to c of f of x,  as x goes to c from the left of f of x equals the limit as x goes to c from the right of f of x equals L. So this kind of looks like the theorem we proved about Limb Soup and Limimf, but they don't have any connection.  So, OK, kind of what, if you want to make some sort of connection between the way this theorem looks and the statement of the theorem for liminf and lim sup, so this is kind of saying that limit of a function equals L if we approach from the left or right, the function f approaches L, OK? And for the lim sup liminf guy that we did for sequences, you could take that as saying that the limit of a function equals L if we approach from the left or right, the function f approaches L, OK? And for the lim sup liminf guy that we did for sequences, you could take that as saying that the limit of a function  sequence equals L, if and only if kind of following the sequence from below, that approaches L, and following the sequence from above, that also approaches L. So there's kind of two directions there, just as there's two directions here, but not really. So let's give a quick proof of.  The theorem, it's not difficult if follows almost immediately from the definitions.  should, assuming this, and proving this, should be pretty clear, OK? If I have this, this means that, you know, if I want to be close to L, I just need to be close to c. And therefore, it doesn't matter if I'm close to c from the left or right, I'll be close to L. And going this direction, this says I just need to be close to c from the left, sufficiently close, and I need to be close to c from the right,  the closer to the close to be close to L. So really, there's not a lot of trickiness in the proof. It's just writing these things out. And so I'm just going to write out one direction and leave the other direction to you. So let's assume that the left limit is  that equals the right limit, equals L. And now we want to show that the limit as x goes to c of f of x equals L. OK? Now we want to show limit as x goes to c of f of x equals L. So let's go back to the definition. Let epsilon be positive. We want to be able to find a delta.  so that f of x is within epsilon to L, if x is within delta to c. And what's the point here is c? And here's L. So this is the picture that goes along with this. Assuming these two limits equal L, I know that if that there exists a delta 1, so that if I'm in a limit,  I'm in this interval, then, and here's L plus epsilon, L minus epsilon, then if I'm within delta 1 to c and to the left, then f will, f will be close to L.  to L in that interval. And then since the limit as x goes to c from the right equals L, there exists some delta 2, so that if I'm in this interval, then I'll be, again, close to L. OK? But this means that if I choose the smaller of these two, and I look at the whole interval, then f will be close to L on the whole interval, and that's it. OK? So let epsilon be positive.  since limit as x goes to c minus f of x equals L, this implies there exist delta 1 positive such that if x minus c is less than delta 1, then I get that f of x.  If c minus delta 1 is less than x is less than c, this implies that f of x minus L is less than epsilon, and similarly for the right limit. Since limit as x goes to c plus of f of x equals L, this implies by the definition there exists in delta 2 positive, such that there's a limit.  that if c is less than x is less than c plus delta 2, then I get f of x minus L is less than epsilon. Now, choose delta to be the minimum of delta 1 and delta 2. And then we'll now  show that this delta works. OK, so now we're going to show this delta works. So if it's less than delta, then this implies that  So if we take something close within delta to c, and delta is the minimum of these two distances, then for the sake of this picture, let's say delta 1 equals delta 2. So now I'm looking at this interval, then two cases, either x is in c, and delta is a minimum of these two distances, then for the sake of this picture, let's say delta 1 equals delta 2.  c minus delta c, which is a subset of c minus delta 1, c, since delta is the minimum of those two deltas, which implies, by the first inequality here for delta 1, or x is in c, c plus delta, which is a subset of c.  c plus delta 2, which by our choice of delta 2, gives us that. Thus, we've shown that if x minus c is less than delta, then f of x minus L is less than epsilon. That's the end of the proof.  I've said this over and over again. Limits, the functions don't care about what the function is doing at the point. It cares about what the function is doing near the point. Now, we're going to discuss the notion of continuity, which connects the limit of a function at a point to the function of a function  And so it connects how a function behaves near a point to the function evaluated at the point. OK? And so let me even write this down. How a function behaves near a point.  So near x. And you'll see from writing, when I write down the definition, basically, it's what's kind of staring you in the face is that the definition  of continuity is that the limit as x goes to c of f of x equals f of c. So we had these examples where the limit, I think already erased it, but where the limit exists but does not equal f of c, and here for continuity, the notion of continuity is that the limit as x goes to c of f of x actually equals the function evaluated at that point.  following definition, let S be a subset of R, and see an element of S. Let me say f is continuous at c if for all epsilon positive, there exists a delta positive such that,  for all x and S satisfying x minus c is less than delta. So in particular, look, I can now, for example, x equals c will satisfy this inequality. I don't have the 0 is less than that. So for all x and S, which are close to c, within delta, I have that f of x will be,  with an epsilon of f of c, OK? So in this case, so just a little, if f is continuous at every point on its domain that we're considering, we just say f is continuous.  OK. So for a function to be continuous at a point, nearby x, x being  near c should mean that f of x should be near f of c. OK? And we'll go through, let's go through some examples. Remember, whenever you get a definition, you should look for examples. And then potentially negate it. We'll negate this definition in just a second to show that a function I wrote down a minute ago is  not continuous. So the function, so the affine function, f of x, equals a times x plus b, x, so S is R. So x is a real number, is continuous function, meaning it's  continuous at every real number c. So let's prove this. Let c be an element of R. We want to show f is continuous at c. So we have to go through the definition. Epsilon be positive. Choose delta to be epsilon over 1 plus epsilon value of a.  last time in the previous lecture, I kind of gave the intuition on why you would choose this delta based on the function and epsilon. I get a computation here. I'm just going to choose delta this way and you'll see that it works. So now we have to show this delta works. If x minus c is less than delta, we should be able to now show that f of x minus f of c is less than epsilon.  This is equal to ax plus b minus ac plus b. So this is equal to a times x minus c, which equals absolute value of a times absolute value of x minus c. This is less than delta. Absolute value of x minus c is less than delta times a, which equals absolute value of a over 1 plus the absolute value of a times epsilon.  which is less than epsilon. OK? Because a number over 1 plus that number is always less than 1. OK? Maybe you were wondering why didn't I just choose delta to be epsilon over the absolute value of A. This is just a smidgen of sophistication that what happens if the absolute value of A is equal to 0, then we would have divided by 0. So adding a 1 there takes care of that.  OK, so this guy is continuous at every c, so this function is continuous. So I've got a function that's not continuous at a point.  make sure this is the next topic. Yeah. Like here is a non-example. The function f of x, which equals 1 if x equals 0,  2 with x not equal to 0, so that's the function f is not continuous at 0.  So this, let's negate the definition of continuity. So the negation of continuity is, so f is not continuous at c if, so the for all's become their exist, and their exists become for all. So if there exists some bad epsilon,  So that for all delta positive, there exists an x such that x minus c is less than delta, and we do not have, the second inequality f of x minus f of c is less than delta.  bigger than or equal to this bad epsilon. OK? Now for this guy, it's pretty clear which x to choose. So let's think this out for a minute. There should be some bad epsilon 0 so that if I take any small interval around 0, I can find a point in this interval so that f of x minus x is less than epsilon.  f of c is going to be bigger than or equal to epsilon 0. Now here, f of c is f of 0, which is 1. Now, what would be the bad epsilon so that f of 1 is greater than distance 1 or greater than distance epsilon 0 to f of x for some x in this interval? Well, if I take any x in this interval other than 0 and stick it in the end,  into f, I'm going to get two. And that's with greater than or equal to distance, 1 to f of 0. So epsilon 0, I will choose to be 1. OK, so now we want to prove that f, this function here, is not continuous at 0. So I'll tell you what the bad epsilon is, choose epsilon 0 equals 1.  So now we have to show this bad epsilon 0 is indeed bad. The delta would be positive. We have to now find a number in this interval so that f of x minus f of c, I mean, f of 0, is bigger than or equal to 1. And like I said a minute ago, if you take any x in this interval other than 0 and stick it into f, I get 2, which is distance 1 to f of 0.  x to be delta over 2, say. Then x minus 0 is less than delta. It's actually equal to delta over 2. And f of x minus f of 0, this is equal to 2 minus 1, which is bigger than or equal to epsilon 0.  So this function is not continuous. And so next time, and I'll just leave this question here, which we'll address in the next lecture, but it's a kind of simple question. So first off, if you look at this function, it shouldn't be too hard to convince yourself. So you're also told when you were a child that a function  is continuous if you can draw the graph and not lift up the pencil, which I better not see on. The exam, if I ask you about continuity, but anyways for the sake of this conversation, let's take that as the intuition. So you can convince yourself that this function is continuous over here, though, right? If I'm getting close to, let's say this is minus 1, then the function is getting close to 2, and the function is less than 1,  value of the function at minus 1 is 2. So the function is getting close to the value of the function at minus 1. And the same for, if I'm looking at c equals 1. So you should be able to convince yourself that this function is continuous at every point other than at the origin 0. So a natural question to ask is, let f be a function, let's say defined on the whole real number line.  line does there exist a point in R such that f is continuous at this point c. For this example over here, we were able to any point other than 0, the functions continuous there. So natural question is, let's say I take  make an arbitrary function, does it have to have a point where it's continuous? And next time we'll see that that answer is no. We'll give an example that's, I think, due to Dirichlet because it's named after him, but naming doesn't necessarily mean anything in math. Green's theorem is named after green, but he didn't prove it, so maybe it was due to somebody else, and we'll use a similar characterization of continuity.  that's kind of analogous to this first statement we had for limits. And we'll do that next time.