 OK, so at the end of last time, let me just restate a theorem that we proved, which was the
viaarstrass  M test, which I'll state in brief or form now as the following, suppose f j are a sequence of functions from some subset S of R, so that for all j, there exists
none-negative number M sub j such that 2  things hold, first off, these M sub j's dominate the f sub j's, f sub j of x is less than or equal to M sub j, and 2, these M sub j's are  then the conclusion is the sequence of partial sums, so converges uniformly. So there exists some function f from S to R so that the partial
sums converge uniformly.  So this is a sequence of functions, delta by the f j's by just taking the sum of
the first j equals 1 to n. And so for example, and I went through this last time, so example, if f j of x equals our old friend  1 cosine of 1, 60, j, x over 4 to the j, then n, let's say on R, so S is R, then a couple of things, f j of x for all x is less than or equal to 4 to the
minus j  because cosine of whatever you put into it is
always bounded by 1, and since this is converges, this implies that this series, which I'm writing this way, which you should think of as a limit  of functions of the partial sums, which are functions, converges uniformly on R. Not only that, this function defined by I take x, I stick it in here, is continuous, because last time, we proved that the
uniform limit of continuous functions is  continuous, this function here is the uniform limit of the
partial sums, which is just finitely many cosigns of things, so that's continuous. So this gives another proof of one of the things we did by hand, when we spoke about differentiability, namely that this function that we considered was continuous.  go back to power series, which was our original
motivation. So I'm going to be saying that a series, so let me, again, make this perfectly  here, when I say, if you like, make this a definition, or really this is just, I was using this terminology up there in that theorem, and when I was talking about this
example afterwards, but when I say a series involving functions converges uniformly, I mean the partial sums converge uniformly.  So, means there exists a function f such that, and all of these are from S to R as well, such  just so that there's no ambiguity. So if I have a sequence of functions f sub j, and I form their series, and I say that
converges uniformly, that means there exists some function f so that the sequence of partial sums, these are now functions, converges uniformly to the function f.  can think of them as a series involving functions, so let me just state the following theorem about when we have a
uniform convergence.  with radius of convergence, R0, which I will recall is defined to be the limit as j goes to infinity, a j 1 over j inverse.  of convergence is infinity. So if this is a finite number, this is a
meaningful expression, if this is 0, then this is short-hand for saying the radius of convergence is infinity.  I have uniform convergence. This is the statement of the theorem, then for all R in 0, 0, R0, this power series now
thought of as a series of functions of x converges  uniformly on x0 minus R, x0 plus R. So the picture is, we have some radius of convergence, x0, x0, x0 plus R, x0 minus R0, if we take any closed interval
interval, basically, inside of that, strictly inside of that interval  then we have uniform convergence of the
properties of the power series. So we'll prove this using the Viastras M test.  then for all j, and so the y-stress M test is for j equals starting from 1 and going to infinity, but you don't have to just for series, we don't have to start at 1,
that can start at 0, so for all j, union 0, what do we have?  this interval, where we want to show uniform convergence, we have that this function a sub j, x minus x0 to the j, in absolute value, what is this bounded by?  so R to the j. So this will be my mj that I'll use for the
veyor's-strawsum test test. Let me put that in parentheses, because I may not use that mj. So these individual functions, just polynomials are bounded by this number for each j,  And now, I want to see if the series of involving these
numbers converges, so let me apply the root test, we have  this is equal to, now this r to the j to the 1 over j just becomes R, and it can just come out of this
limit, and I pick up a to the 1 over j, and this is equal to 1 over R, so this is equal to, if you like, I can put R over R,  R is less than infinity, meaning if the radius of
convergence is less than infinity and 0, and what do we notice? Now, R is less than R, R is coming from the closed interval 0 up to the R0, not including R0. So this number, so this is always less than 1, but this number is always less than 1, 2 as long as R is less than R, which implies that the series,  from j equals 0 to infinity of a j, R to the j converges. So now we have the sequence of functions, the polynomials that we use to build up our power series, each of them bounded by this number, these numbers, a sub j, absolute value, times R to the j, and these numbers are
submissible, the series converges.  M test, this implies that the power series converges uniformly on this interval.  OK, all right, so as long as we stay within, strictly within inside the radius of convergence, we have uniform convergence of the
power series, and therefore, using the theorems that we proved before, we can differentiate and integrate term-by-term the power series  So which follows immediately from this previous
theorem and then what we proved last time, which I'll state now,  for the sub convergence, rho positive, so let me write it this way.  x0 plus R, the function given by the power series, which I'm not going to call this f or anything I'm just going to
refer to the power series directly, this is differentiable at c. And to compute the derivative, you can just do it term by term, meaning I can take the derivative inside,  I d x of j equals 0 to infinity, a sub j evaluated x equals c, this is equal to simply
differentiating term by term.  OK? And number 2, for all a, b, for all a, less than b, well,  x0 minus R0, less than a, less than b, less than x0 plus R0. So here, I'm sticking to, strictly with the inside of the radius of convergence, there's the x0 plus R, x0 minus R0, and so now I'm going to
integrate over some interval inside, then I can integrate term by term, the integral from a to b,  sum from j equals 0 to infinity of a sub j, x minus x0, j dx equals, and I'll just write it this way,
this way, sum from j equals 0 to infinity of, OK?  And in fact, let me just to really emphasize that we're interchange
internaching limits here, let me instead write this equivalently as sum from j equals 0 to infinity of j by d by d x of a sub j, x minus x0, j evaluated at x equals c.  So for a power series, I can interchange the limits, the limit being taking a derivative and the sum, I can interchange, and then I can also interchange the
integration and the sum, as long as I stay within the radius of convergence. And this statement here is a pointy, so let's see why one is the case.  what we've proven, and so this previous theorem, and the theorem we have about
integration and uniform convergence, so what about 1? So first off, we have uniform convergence of the series strictly inside of the radius of convergence, so how about we need to check to see if  the formal derivative also has radius of convergence
as well, equal to R0. So I claim that the radius convergence of the derivatives, which equals  I can write it this way. So the derivative of this guy is j times a sub j times x minus x
0 to the j minus 1. So just shifting indices, this is equal  times j plus 1 times x minus x minus x0 to the j. So our claim is that this power series here has radius of convergence equal to R0, the original radius of convergence. So R0 is a radius of convergence of, why don't I come over here? I still had a board left, sorry about that.  The original power series has radius of convergence row. What we need to show to prove part 1 is that the radius of
convergence of the derivatives of taking the derivatives inside also has radius of convergence row.  and uniform convergence of the derivative of the power series, so by the
theorem we proved in the last lecture, we would have that the derivative of the power series equals the power series of the derivatives.  we compute that if we take the limit as j goes to infinity, now these are the coefficients for the new
power series, so a, j plus 1, j plus 1, raised to the 1 over j, this is equal to limit j goes to infinity  a sub j plus 1, now raised to the 1 over j plus 1, j plus 1, 1 over j plus 1, now I'll raise to j plus 1 over j.  the limit as j goes to, not last time, but way back when we're looking at
sequences, the limit as j goes to infinity of this guy is 1. So this, in fact, equals limit as j goes to infinity, this goes to 1, this goes to 1, so you can actually say this converges to 1 as well.  to the j plus 1 over j. Now, strictly speaking, you would have to, so this thing also converges to what does it converge to?  Well, it's way over there, so let me recall that the radius of convergence
of the radius of convergence, R0, let me put an inverse here, this is equal to the limit as the k goes to infinity of a sub k 1 over k.  So therefore, this is supposed to be the radius 1 over the radius of convergence
of convergence to the differentiated power series, which implies  j is 1 over this limit, 1 over that limit which we computed is to be 1 over R0, which is the original radius of convergence. So all this to say is that the radius of convergence of the differentiated
power series, the formally differentiated
power series is the same as the radius of convergence of the  original power series. And therefore, you have uniform convergence of the
derivatives wherever you have uniform convergence of the original power series. And therefore, by the theorem we proved last time, since both the derivatives and the power series, in the derivative of the power series converges uniformly on the same set,  the infinite sum, so this guy is actually
differentiable, and the derivative of the power series is the power series of the derivatives. But you can iterate this because I now have this power series with radius of convergence equal to the radius of convergence of the original, and then I can take a derivative of that and show that has the same radius of convergence as the original  So let me just leave this as a remark and not stated as a theorem, iterating, can prove that I want the k-th derivative of the power series.  So let me say, I'll x in here, the k-th derivative of the power series is equal to the power series of a differentiating term by term.  So this holds for, OK, equals 1, 2, and so on. And so in particular, if I evaluate at x0, this tells me that k factorial, a sub k is equal to the derivative of this function, which you get out when you stick x
into this power series  And you can interpret this, although we never call them Taylor series as a statement that every power series is the Taylor series of a function, at least in this setting that we're looking at.  OK, so we've answered pretty definitively, at least for the scope of this class, when we can interchange limits, we can do that as long as we have uniform convergence of the
objects that we're interested in, be it  the continuous functions or function n, it's derivative, but there are more powerful
statements out there that one can make, especially when it comes to integration, this is why essentially why a different theory of integration was created, or one reason why.  But hopefully, if I see some of you in 18102, which is what I'm
teaching next semester, we'll get into that further when we discuss La Baye integration, that was thought of because somehow, Riemann integration is not complete, the same way that the rational numbers are not complete.  And the big integration is, the big integration is, complete in a certain
certain sense, I'm being very vague here for a reason. So I'm just giving you where you can go with this, what the next step is, at least in proving when can you interchange two limits, is really a topic that's fundamental to the study  study of the big integration, and you have much more
powerful theorems there than you do here, which allows you to prove interesting results, and especially about Fourier series.  I want to now prove the last theorem of the class, which is also due to the godfather. So when we had these power series, so these are
defining functions, very special types of functions,  what are called analytic, they are, by definition, essentially, the limit of polynomials. They're the limit of these finite sums of a sub j's times x minus x minus x0, raised to the j power that's polynomial. So for analytic functions, which are these functions equal to power series, they are the limits of
the limit of polynomials.  else, but that's a pretty small class of functions, analytic functions, but the very interesting result that actually something like this is true for all continuous
function, so roughly speaking,  Basically, every continuous function is in some
sense, almost a polynomial. Just as we saw for these analytic functions, meaning defined by power series, they are very close to being, at least within their radius of convergence, a polynomial.  And in what sense do I mean that? So this is a barrage-strassist approximation theorem, which states the following, if f is in a continuous function on the
unit interval, say you can make it a, b just  rescaling the variables, but I'll just stated for continuous
functions on the unit interval, then there exists a sequence, polynomials, p sub n of x, such  p sub n converges to f uniformly on 0, 1. And so in this sense, every continuous function is well
approximulated by polynomials. So every continuous function is close to being a polynomial.  OK, so I need to first prove a couple of things that will be needed. So first off, I'm only going to look at only, so let me make some remarks before we go to the proof. So we're putting the proof on hold now. So let me just make a remark.  for every f, just for certain f, and then for every f, we'll
follow from this special class. So we only need to consider, we'll only consider the case, f of 0 equals 0, and f of 1 equals 0, so f is 0 at the endpoints.  so that we can extend f to a continuous function
function outside of the unit interval by just setting it equal to 0. And so let's suppose we've proven this special case, and we look at the general case,  what do we know that there exists a sequence of polynomials p sub n such that these polynomials, p sub n, converge uniformly to a small
modification of f that results in  So this function here, if I look at it, it's a continuous function on 0, 1, because I'm just
modifying it by constants and then times x, and then at f of 1, I get f of 1 minus f of 1 minus f of 0, minus f of 1 minus f of 1, minus f of 0, so I get 0, and at 0, I get f of 0,  that don't care equals 0. So there exists polynomials converging to this function now. If we've been able to prove the case just for f of 0 equals 0,
equal to f of 1 equals 0, and f of 1 equals 0. So uniformly, and therefore, the polynomials given by p sub n of x plus, now x times f  minus f of 0 plus, so this is still, if this was a polynomial, so is adding this to it, that's
still a polynomial converges to f tilde of x uniformly, and this, again, this is still a polynomial.  we need to consider the case f of 0 equals 0, and f of 1 equals 0. And we're going to do that just so that we can
extend the f to a continuous function outside the interval.  converged to f is by what's called an approximation to the
identity. Really, I guess you could think of it as an approximation to the delta function, and I'll explain that in just a minute.  dx, 1 over, and then Q sub n of x, this is going to be equal to c sub n times
1 minus x squared raised to the n. Then a few simple consequences are. So first off, this is the integral of a function which is not negative, but it's positive at plenty of places  minus 1 and 1, and you proved in the homework that this means that the
interval has to be positive, then the first is for all n, a natural number, for all x and 0, 1. So the first two of these observations are very clear, Qn of x is greater than, so minus 1 to 1,  2n of x is bigger than or equal to 0, so it's just c sub n, which is a positive number
times 1 minus x squared, x squared, so x is between minus 1 and 1, and therefore, 1 minus x squared is always non-negative. 2 is, this is also pretty clear, the integral for minus 1 to 1 of c sub n of x d x equals 1.  Now, why is this clear? Because this is equal to the integral of 1 minus x squared
race to the n times c to the n, or c sub n. But c sub n is the inverse of that integral, so we should just pick up 1. Now, the third, and less trivial thing, which is important, is that for all delta,  1, this function Q sub n, I mean, this is just a polynomial, converges to 0 uniformly on the set delta is less than or equal to the
sub n, the absolute value of x delta is less than or equal  the absolute value of x is less than or equal to 1. So in other words, here's minus 1, 1, 0, delta, minus delta, if I look in these regions, so in the, if you like, the union of those two intervals, then k sub n, this polynomial is converging to 0 uniformly, as n goes to infinity  on the union of these two intervals. So what is the picture of what's going on? What do these
cute sub n's look like? So here's minus 1, here's 1, so kind of the first one looks like some constant times 1 minus x squared, so there's the first one,  and keep getting bigger and bigger, this is 0 at higher and higher
order at 1 and minus 1 and getting pretty small near here, and in fact, according to 3, if I take any interval around 0 and look outside of it as n goes to infinity, Q sub n is going to 0 uniformly, so what it should look like is, maybe the next one,  like that, so that if I look inside over here or over here, Q sub n is going to 0 uniformly. So what you should think of is that the Q sub n's as n goes to infinity is something like a
directorial delta function, which is not exactly a function.  So again, so let me re-emphasize that, and this is in quotes, you should think of Q sub n, x like delta function.  at x-centered at x equals 0. So that's in quotes because that is meaningless. But some of you have taken physics, no, what properties are of a delta function. And the integral is 1, it's somehow 0 everywhere away, except for the origin and it's infinite there, but somehow
is
interval 1, so that's  OK, so these Q sub n's will show form an approximation to the
identity in a certain sense, but let's prove the only non-trivial 1, which is 3, number 1 and 2, or clear, based on how they're defined.  So let's first estimate how big as this, so this constant c sub n, we don't really know what it is,
explicitly, but let's compute at least a rough size of it.  a natural number, the following inequality in xn minus 1, 1, 1, 1 minus x squared raised to the n, this is greater than or equal to 1 minus n x squared. Now, if it's not, so it shouldn't be like, you know, hit you in the face clear why this is true, but one way you can prove it is that if you look at the function  x equals 1 minus x squared, and so first off, this inequality is even in x, doesn't matter if x is
negative or positive, so let's look at this function on 0, 1, then what's the point? If I look at g of 0, this is 0, and if I compute g  prime of x, this is equal to n times 2x times what? times 1 minus x square to the n minus 1, and this is always less than or equal to 1, this thing in parentheses, so this thing is always bigger than or equal to 0 on 0, so on 0, 1, this function  increasing and its value at 0, 0, 0, so we get that, which is exactly what we wanted to prove.  If I want an upper bound on c sub n, I need to prove a
lower bound on 1 over c sub n. So let's take 1 over c sub n and find a lower bound for it. This is the interval for minus 1 to 1 of 1 minus x squared raised to the n, the x. Now, I can't remember if I made this a homework problem or not, but for even functions,  the origin, this is just twice, I mean, I'm sure you remember this from calculus is not hard to prove with what we know the change of
 variables, so on that this is equal to twice times the interval from 0 to 1 of 1 minus x squared raised to the n. Now, this is bigger than or equal to 2 times if I integrate to a certain point, this certain point is chosen so that I get a  is pretty, essentially. Now, now, this is where I replace this by the smaller thing, which is
easy to integrate. So this is greater than or equal to 2, and it goes 0, 1 over root n, 1 minus n  x squared, dx, and I leave it to you to verify that with this choice at the end point, what I get is
for over 3, root n, 1 over root n. And therefore, and this is bigger than 1 over root n.  started with 1 over c sub n, and I showed that it was bigger than 1 over root n, and therefore, c sub n is less than 1 over, c sub n is less than the square root of n. Now, we'll use this to compute what we want.  some polynomial and n, but this will suffice. So we now want to show, so let delta be positive, we now want to show that c sub n converges uniformly to 0 on that set where the absolute value of x is less than or equal to 1 is
or equal to 1 is bigger than or equal to delta.  We note that the following sequence converges to 0 that square root of n times 1 minus delta squared raised to the n, so we should also put delta's n 0, 1.  converges to 0 as n goes to infinity. Now, intuitively, why is this? This is because this is number less than 1, raised to the n-th power, exponential always beats just a power of n. If you want to see exactly why this is, we could compute the limit as n goes to infinity of this sequence  raised to 1 over n power, then this is equal to the limit as n goes to infinity of n to the 1 over n, 1 1 1 1, 1 minus delta square, and this converges to 1, we proved that, and so this equals 1 minus delta square, which is less than 1, and we have this theorem from  section on sequences that says if this limit is less than 1, then the thing here converges to 0, or you could interpret this as saying that the
series with this as the individual terms converges, and therefore, the individual terms have to converge to 0.  equals 0. So now we have this, and once we have this, we'll have what we want. So we want to prove uniform convergence on that set, so let epsilon be positive.  square root of n times 1 minus delta square,
or raised to the n is less than epsilon, and for all n bigger than or equal to m, for all x so that delta is between less than 1, we get that. Q sub n, so it's not negative, minus 0, an absolute value  these sub n of x, a cn, this is less than or equal to square root of n, that's for this guy, 1 minus x squared, this is getting smaller as I get closer to 1, so it's
bigger at delta, and how we chose m is, this is less than epsilon.  OK, so now we're ready for the proof of the
veyorstrass-strass approximation theorem.  f of 0 equals 0, f of 1 equals 0. So this polynomial here is very concentrated at 0.  Well, first off, suppose f and its continuous here is here. So you can check that if it's 0 there and I extend it to be 0 outside of 0, this is still a continuous function. I'm just defining it this way because I want to write certain symbols in a little bit without
specifying exactly where the
bound of the integration R.  So we extend f by 0 outside of 0, 1, and this function f is continuous function on the real number line.  So we define, now we're going to define this polynomial, sequence of polynomials, p sub n, this is going to be equal to the
intergrowth from 0 to 1 of f of t times Q sub n of t minus x dt, and just a reminder, this is equal to the interval from 0 to 1 of f of t times c sub n  in this x minus t squared raised to the n dt. So this is just c sub n times this thing, if you expand everything out, is just using the binomial theorem, this is equal to j equals  I'm just going to put a sum here, meaning a finite sum. Some numbers, a sub j n times x to the j times
t to the j times t to the k, all right?  if you expand this out using the binomial theorem, you get this polynomial in x sub j, t sub k. And then this is getting
integrated against f of t, dt. And so this is, in fact, a polynomial.  your convince that it's a polynomial, this is equal to the
intergrowth from 0 to 1 of f of t times c sub n, now we use the binomial theorem, this is equal to j, 0 to n, n choose j, and then minus x minus t squared  dn minus j, or I could put j, 2j, and then dt, I'm not sure if this is even helping or anything, but just so that you see this is
actually polynomial, f of t  c sub n, sum from j equals 0 to n, and now, sum from k equals 0 to 2j, n
choose j minus 1 to the j, now 2j choose k minus t to the k times x to the 2j minus k,  and then dt. So I have all this junk, integrated dt, and then this is x to the 2j minus k, so that just pops out. So this is a polynomial.  outside this integral times, f of t times, you know,
integrated it against minus 2 to the k. I think I said more than I needed to there, so the point is p sub n of x is a polynomial.  this as it was before, now this we change variables and is equal to, so we do use substitution,
now where u is equal to x minus t, and then I'm going to change  dT, so what I did here was a change of variables, I said, and
u is equal to x minus t, if you like, t minus x, and then I just recall, and then I just called  So I'm eventually looking at these polynomials only in the interval 0, 1. So this is what it looks like.  So I can extend the integration to minus 1, and 1, again, with the understanding that I've extended f to be 0 outside of 0, 1.  this way, since f of x plus t equals 0 for t, not n minus x and 1 minus x.  So this is a little discussion. So I said that, and this is for those who understood my
comment about delta functions, in Q sub n is very concentrated at 0, at t equals 0.  you should think of as being approximately like a delta function at t, if you don't know what a delta function is or
never heard of it, then forget the rest of this remark. And then I'll say something. So Q sub n kind of looks like a delta function, and therefore, minus 1 of x plus t, Qn of t, dt,  look more and more like f of x plus t, delta of t, dt, and what we know about
direct delta functions is that when you integrate them against a function, you just pick up the function evaluated at 0, which is, that's why you expect it.  this picture of what the Qn's are looking like, they're concentrating more and more at 0. So all of the
contribution to this integral here, which defines these polynomials, is happening at t equals 0, at t equals 0, I just pick up f of x. So that's why you should kind of expect these polynomials to converge  back to the function. So the thing I get to finish on this board, so this is a picture you should have
seared in your mind, this is what the Q sub n's look like that they're constant  all of their mass right at the origin, and therefore, I should just pick up f at the point x. Since this is
concentrating at the origin, t equals 0, I should just pick up f of x plus t at t equals 0, which is f of x. Why f of x plus c not some constant times f of x plus t, that's because the integral of Q sub n's is 1.  So now let's prove that the pn's converge to f uniformly.  since f is a continuous function on a closed and bounded
interval, we know that it's uniformly continuous. And therefore, there exists delta positive, such that,  such that if, sorry, this way, for all zy so that z minus y less than delta, we get that f of z minus f of y is  Actually, we don't need, so I mean, we don't need so much about f, but we'll go with this anyways, we really just know, and never mind, I'll stop. So we know that f is uniformly continuous, so there exists a delta, so that I have this, so less than epsilon over 2.  So if z and y are within delta to each other, then f of z and f of y, no
no matter what z and y are, within epsilon over 2 of each other, now, since f is continuous, it has a maximum on this interval, or I should say, since f is a continuous function, it has both a max and a min on this interval  there exists some number c such that f of x is bounded by c for all x and 0, 1.  coming from the fact that it's bounded on this interval, and now I'm going to choose my m for the
uniform convergence of the polynomials, just depending on these pieces of input, then as we showed, since square root of n, 1 minus delta square n converges to 0, this implies there exists an m,  n bigger than or equal to M, I get that square root of n, 1 minus delta
delta squared, raised to the n is less than epsilon over 8c.  minus f is less than epsilon, and for all n bigger than or equal to M
for all x in 0, 1, if we look at p sub n of x minus f of x, we're going to use that this thing is an approximation to the identity,  essentially that it satisfies those three properties that I
wrote a minute ago, so this is equal to the integral for minus 1 to 1 of f of x plus t times Qn of t dt minus f of x, what's the integral for minus 1 to 1 of Qn sub t, that's equal to 1, so f  x is at times, so this is all of analysis, this is writing 1 or 0 in a certain way, not all, so I can write this as f of x plus t, now minus f of x times
cue n of t, dt, again, because the integral of c sub n of t equals 1, so again, if you just  this out, this is f of x and integrating with respect to
t, so I just pick up what I had before, now by the triangle inequality for intervals, this is less than or equal to the interval for minus 1 to 1 of f of x plus t minus f of x times the absolute value of c sub n of t, but since c sub n of t is non-negative, that's just c sub n of t.  going to split this interval up into two parts, this is equal to a part where
t is less than or equal to, so should minus delta, delta f of x plus t, minus f of x, Q sub n of t dt, and then plus the other part, which I can write  So it's a union of the two intervals now away from delta, and
out to 1. So it's the sum of the integral over these two intervals, which I'm going to write as the integral over delta less than or equal to t is less than or equal to 1,  times Qn of t, all right? Now, t is between minus delta and delta, and therefore, x plus t minus x, an absolute value is less than delta.  here, x plus t minus x equals t is less than delta in this here. So since this guy minus this guy is less than delta in absolute value, I can use this uniform continuity part to say that this is less than epsilon over 2,  plus, now, the absolute value of this guy is less than by the triangle
inequality is less than or equal to the sum of the absolute values, which is less than or equal to c2c.  of the absolute values, which is bounded by each of those
bounded by c. Now, this is less than epsilon over 2, integral from if I just integrate the whole thing from minus 1 to 1, that just gives me 1 plus 2c times Qn of t, on this interval is, again, so I should put delta there.  2 c, c sub n, 1 minus delta squared, raised to the n, dt, and so there's no t here, so this is equal to epsilon over 2, because this interval is 1, plus this  c sub n, remember, remember, is less than the square root of n times 1 minus delta square root of n times
1 minus delta square root of n times the integral over this region dt, which I can make bigger by going for minus 1 to 1, it gives me 2, and this is equal to epsilon over 2 plus 4 c square root of n, 1 minus delta square root of n, and this is less than epsilon over 2 plus epsilon over 2 equals epsilon.  this was quite an experience, teaching to an empty room. I hope you did get something out of this class, unfortunately, I wasn't able to
meat a lot of you, and that's one of the best parts about teaching and being able to see you grasp in real time what I'm talking about. So hopefully this nightmare will end soon, and we'll get to see each other in the future.  you