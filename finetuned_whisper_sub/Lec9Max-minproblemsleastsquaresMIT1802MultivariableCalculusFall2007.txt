 The following content is provided under a Creative Commons Commons Commons license. Your support will help MIT open courseware continue to offer high-quality educational resources for free to make a donation or to view
additional materials from hundreds of MIT courses, visit MIT open courseware at ocw.mit.edu.  to kind of minimization, or maximization,
problems involving functions of several variables. So if all that, so remember, last time we say that when we have a function of 2 variables x and y, then we have actually two different derivatives,  And we have a partial f, partial y, also
called f sub y, where we vary y, and we take x as a constant. And now, one thing I didn't have time to tell you about,  about the oxidation yesterday is the approximation of the
apoximation of formula, that tells you what happens if you value both x and y. So f sub x tells us what happens if we change x a little bit by some small-termount delta x. f sub y tells us how f changes if we change y by a small-termount delta y, if we do both at the same time, then the two  because you can imagine, but first two will change x and
then you will change y of the other way around, it doesn't pretty matter.  and that changes by an amount, which is approximately f sub x times delta x, plus f sub y times delta y. And that's one of the most important formulas about partial derivatives. So the intuition for this, again, is just the two effects at up, if I change x by a small
mute amount, and then I change y, well, first changing x  will modify f, how much does it modify f, the answer is the rate of change is f sub x. And if I change y, then the rate of
change of f when it change y is f sub y, so all together, I get this change in the value of f. And of course, that's only an approximation from your actually, there would be higher order terms involving second and third derivatives and so on.  So I was distracted by a microphone for the problem.  So let's think about the tangent plane to the
graph of a function f. So we have some pictures to show you, it will be easier if I show the pictures.  saying the graph of f by a plane, that's parallel to the x is a plane, and then when I change x, z changes, and the slope of that is going to be the
dereity of respect to x. So now, if I do the same in the other direction, then I will have similarly the slope in a slice now parallel to the y-z plane that will be partial f, partial y.  and that line is tangent to the surface. So now, if I have two lines tangent to the
surface, well, then together, they determine for me the tangent plane to the surface. So let's try to see how that works.  to be this plane, so I do tension up 9's to the graph, and let's write down the
equations of these 9's, I'm not going to write parametric equations, I'm going to write them in terms of x, y, g coordinates. So let's say that partial f of our partial x at the given point is equal to a, then that means that we have a line given by the following  conditions, so I'm going to keep y constant equal to y0, and I'm going to
change x, and does I change x, z will change at 0, 8, that's equal to a, so that would be z equals 0 plus a times the change in x, x minus x minus x0.  that's the intersection with a slice parallel to the x is equal to the x is a
play, I hold y constant equal to y0, and z is a function of x that varies with a rate of a. And now if I look similarly at the other slice, let's say that the partial with respect to y is equal to b,  by the fact that z now will depend on y and the rate of change with respect to y
will be b, while x is held constant equal to x0. So these two lines are both going to be in the tangent plane to the surface.  So they're both tangent to the
half of f, and well together, if they're coming to a plane, and that plane is just given by the formula z equals  c0 plus a times x minus x0 plus b times y minus y0. If you look at what happens, so this is the equivalent of a plane, g equals constant times x plus constant times y plus constant, and if you look at what happens if I hold y constant and
x, I recover the first line, if I hold x constant  and then very why I get the second one. Another way to do it, of course, would be to write, actually, the
parametric equation of these lines gets vectors along them and then take the cross product to get the normal vector to the plane, and then get this equation of a plane using the normal vector. That also works, and it gives you the same formula. So if you're curious, exercise, do it again, using
parameter, using
parameter, and using cross product to get the plane equation.  So that's how we get the tangent plane, and now what this approximation
of a similar here says is that, in fact, the graph of a function is close to the tangent plane. If we were moving on the tangent plane, this would be an actual equality, delta z would be a linear function of delta x and delta y. And the graph of a function is near the tangent plane, which is not quite the same, so it's only an approximation for small delta x and small delta y.  So the approximation of the
proximation formula says the graph of f is close to its tangent, and we can use that formula  Now, to estimate how the value of f of f change is, if I change x and y at the same time. Questions about that? Now that we've caught up with what we were supposed to see on Tuesday, I can tell you now about
max and min problems.  So that's going to be an application of partial derivatives to look at
obtannization models.  So maybe 10 years from now, when you have a real job, your job maybe to
atually minimize the cost of something or maximize the profit of something, but typically, the function that you will have to strive to minimize or maximize will depend on several variables. So if you have a function of one variable, you know that to find its minimum or its maximum, you look at the derivative and you set that equal to 0 and you try to then look at what happens  of a function. So here it's going to be kind of similar, except, of course, we have
separal derivatives. So for today, we'll think about a function of two variables, but it works exactly the same if you have three variables, 10  So the first observation is that if we have a local minimum or a
local maximum, then both partial derivative, so partial f, partial x and partial f, partial y, are both 0 at the same time. So why is that?  x is 0, that means when I vary x, well, to first order of a function
of the function doesn't change, maybe that's because it's going through, if I look only at the slice, parallel to the x axis, then maybe I'm going through the minimum.  so myself to change y, that doesn't work. So I need actually to know that if I change y, the value
will not change either to first or there, so that's why you also need partial f, partial y to be 0. Now, let's say that they're both 0, why is that enough? It's essentially enough because of this formula telling me that if both of these guys are 0, then to first or the other of a function doesn't change, and then, of course, there will be maybe quadratic terms that will actually  say that your function is actually constant, it will just tell you that maybe it will be
actually quadratic or higher order in delta x and delta y, that's what you expect to have at the maximum  to the graph is actually going to be horizontal. And that's what you want to have, say you have a minimum, well, c that the attention plane at this point, at the bottom of the graph, is going to be horizontal.  And you can see that on this equation of a tangent plane, when both these coefficients are 0, that's when the
equation becomes z equals constant, the horizontal plane. Does that make sense? So we'll have a name for this kind of point, because actually what we'll see that as soon is that these conditions, necessarily, but they're not sufficient, there's actually other  the partial derivatives are 0. So let's give a
name to this, we say, the definition, we say that x, let's say x0, y0, y0, is a critical point of f  the partial derivatives with respect to x and the partial
dereverative respect y are both 0. marginally, you would want all the partial derivatives, no matter how many variables you have, you want all the partial to be 0 at the same time.  So let's say I give you the function f of xy equals x squared minus 2y plus piy squared plus 2x minus 2y. And let's try to figure out whether we can minimize  x minus this. So what would start doing immediately is taking the
partial derivatives? So what is f sub x? It's down with 2x minus 2y plus 0 plus 2. Remember that y is a constant, so this differentiates to 0. Now, if we do f sub  OK, that's going to be, well, 0, minus 2x plus 6y minus 2. And what we want to do is set these things equal to 0. And we want to solve these two
equations at the same time. So an important thing to remember, maybe I should have told you a couple of weeks ago already, if you have two equations to solve, it's very good to try to simplify  but you must keep two equations. If you have two equations, you should not end up with just
just one equation out of nowhere. So, for example, here, we can certainly simply find things by summing them together. See, if we add them together, well, the x is cancel and the constant cancel, and in fact, we're just left with 4y for 0, that's pretty good,  that then we should, of course, go back to these and see what as we know, well, now it tells us, so if you
put y equals 0, it tells you 2x plus 2 equals 0, that tells you x equals minus 1. So we have one critical point, that's xy equals minus 1 and 0.  OK? Any questions, so far? Well, you should have a question. Question should be, how do we know if it's a maximum
or a minimum? Well, yes, so if we had a function of 1 variable, we would decide things based on the second derivative, and in fact, we'll see tomorrow how to  things based on the second derivative, but that's kind of tricky because there's a lot of
second derivatives. I mean, we already have two first derivatives, you can imagine that if you keep taking partials, you may end up with more and more, so we'll have to figure out, carefully, what the condition should be.  So, in fact, let me point out to you immediately, that there's more than
maxima and minimum. So remember, we saw the example of x squared plus y squared that has a critical point, that a critical point is, obviously, a minimum.  a function, there's indeed a minimum here, but then S where the function
drops to a lower value, so we call that just a local minimum to say that it's a minimum if you stick to values that are close enough to that point. Of course, you also have a local maximum, where I didn't plot it, but it's easy to plot, that's a local maximum, but there's a third example of a critical point, and that's a set of points.  So if a saddle point, it's a new phenomenon that you don't really see in a single
value of calculus, it's a critical point that's a minimum or minimum or maximum, because depending on which direction you look in, it's either 1 or the other. So here, see, the point in the middle at the origin is a saddle point.  you have this
mountain in-pass, at a mountain pass, the ground is horizontal, but depending on in which direction you go, you go up or down. So we say that the point is a set of points  for possibilities could be a local max, or a set of a set. So too, we'll see how to decide which one it is, in general,
using second derivatives. For this time, let's just try to do it by hand. So I just want to, in fact, I can try to, these examples that I have here,  square minus x square, the sums are the distance of squares. And if we know that we can put thing as a sum of squares, for
example, will be done. So let's try to express this, maybe as the square of something. So the main problem is this 2xy, but we know something that starts with x square minus 2xy, that is actually the square of something.  squared, not plus free y squared. So let's try to do that, so we're going to
compete the square. So I'm going to say it's x minus y squared, and then, so that gives me the first two terms and also y squared, what I still need to add two  And I also need to add, of course, the 2x minus 2y. It's simpler, it's still not simple enough, my test. I can actually do better. So this guy looks like a sum of squares, but here I have this x plus 2, but 2x minus 2y, where that's 2x minus y, it looks like maybe we can
modify this,  and make this into another square. So in fact, I can simplify this
fever to x minus y plus 1 squared, c that would start like x, which is here, that would be x minus y squared plus 2y, x minus y, and then there's a plus 1, where we don't have that plus 1, so let's remove it by subtracting 1.  1 here, and I still have my 2y squared. Do you see why this is the same function as that one? So again, if I expand x minus y plus 1 squared, I get x minus y squared plus 2y, x minus y, that's suppose guys, plus 1, but I will have a minus 1, that will cancel out, and
then I have a plus 2y squared.  So now, what I know, this is a sum of two squares minus 1, and this critical point xy equals minus 1, 0, that's exactly when this is 0 and that is 0. So that's the smallest value.  S1 and minus 1 happens to be the value of the
advocate equal point. So it's a minimum. Now, of course, here, I was a very lucky, in general, I couldn't expect things to simplify that much.  if it will be a bit different, but you will see it will also
involve, also, involve, computing squares, just vers more to it than what we've seen.  And that that whole function is greater. How do I know that that whole function is greater or equal to
negative 1? Well, I vote f of xy as something squared plus 2y squared minus 1. This squared, it's always a positive number, non-negative, it's a square.  Similarly, y squared is also always in a negative, so if you add something that's at least 0 plus something that's at
at least 0, and you subtract 1, you get always at least minus 1. And in fact, the only way that you can get minus 1 is if both of these guys are 0 at the same time.  So, more about this to more. So, in fact, what I would like to tell you about, now, instead, is a nice application of min-max
problems that maybe you don't think of as a min-max problem,  because, I mean, you don't think that that way, because probably your calculator can do it for you, or if not your
computer can do it for you, but it's actually something where the theory is based on minimization in two variables. So, very often, in experimental sciences, you have to do something called list squares in  Well, it's the idea that maybe you do some experiments, and you're recurred some
data, so you have some data x and some of our data y, and I don't know, maybe, for example, x is maybe a measuring frog, and you're trying to measure how big the frog leg is compared to the eyes of a frog or what are all trying to measure something.  So how much you put of some reactants, and the output of the output product that you want it to synthesize is generated, all sorts of things, make up your own example. So you measure, for the values of x, what the value of y and the y end up being, and then you would like to claim, well, these points are kind of aligned.  So you want to claim, and in your paper, you will actually throw a nice little line like that, c, these two functions
depend linearly on each other. So the question is, how do we come up with that nice line that passes smack in the middle of the points?  yI, so maybe I should actually be more precise. So you're given some
experimental data, you have a data point x1, y1, x2, y2, and so on, xn, yn.  x plus b that somehow, you know, approximately,
approximates variable this data. So you can also use that, by the way, to predict various things, for example, if you look at your new homework, actually, the first problem asks you to predict how many iPods will be on this planet in 10 years, looking at best sales and how they behave.  lose all the money you don't have yet, you cannot use that to
partically predict the stock market. So don't try to use that to make money, it doesn't work. So one tricky thing here that I want to draw your attention to is what are the unknowns here.  So the natural answer would be to say, well, the unknowns are x and y, but not a true of a case. We're not trying to solve for some x and y. We have some
value is given to us, and when we are looking for that line, we don't really care about the particular value of x. What we care about is actually these coefficients a and b, that will tell us what the relation is between x and y.  nicest possible 9 for these points. So the unknowns in our equations will have to be a and b, not x and y.  OK, so the question really is find the best a and b. And of course, we have to decide what we mean by best.  of a and b that measures the total error that we are making when we are choosing this line
compared to the experimental data. So maybe, hopefully speaking, it should measure how far these points are from the line. Now, there's various ways to do it, and all of them are, I mean, a lot of them are valid, if they give you different answers, you have to decide what it is that you prefer.  a measure of a distance to the line by projecting
projecting perpendicularly, or you could measure instead the difference for a given value of x, the difference between the experimental value of y and the product in 1. And that's often more relevant because these guys are actually maybe expressing different units, they're not the same type of quantity.  Anyway, so the convention is usually, we measure distance in this way. Next, you could try to minimize the
largest distance, say we look at where's the largest error and we make that the smallest possible. The drawback of doing that is, experimentally, very often, you have one data point that's not good because maybe you've fed a sleep in front of the experiment, and so you didn't measure the right thing.  So maybe instead you want to measure the average distance, maybe you want to actually give more weight to
things that have further away, and then you don't want to do the distance, but the square of a distance. So there's various possible examples, but one of them gives us a pretty
particularly nice, formula for a and b, and so that's why it's the universally used one.  of the squares of the errors. And why do we do that? Well, part of it is because, actually, it looks good. When you see this plus in a scientific paper, so they really look like the line is, indeed, the ideal line. And the second reason is because, actually, the minimization
property that we will get is particularly simple, well-posed and it's easy to solve. So we'll have a nice formula for the best a and the best b.  So if you have a method that's simple and
simple and gives you a good answer, then that's probably the good one. So we have to define best, and here it's in the sense of minimizing the total square L, so maybe I should say, a total square deviation,  So what do I mean by that? So the deviation for each data point is the difference between what you have
measured and what you are predicting by your model. So that's the difference between y, y, y, and a, x, i, plus b.  So now, what we'll do is we'll try to minimize the function capital d, which is just the sum for all the
data points of the square of a deviation  So let me emphasize again, this is a function of a and b. Of course, there's a lot of letters in here, but x, I and y, I, in a real life, it will be numbers, given to you, will be the numbers that you have measured.  Any questions? Yes? OK, so how do we minimize this function of a and b? Well, let's use our new knowledge, let's actually look for a
critical point.  And we want to solve for partial d of a, partial a equals 0, and partial d of a, partial b equals 0. That's how we look for critical points. So let's take the derivative of this with respect to a.  to take the derivative of this quantity squared. So remember how we take the derivative of a square, we take twice this
quantity times the derivative of what plus squared, so we get 2 times minus a, sorry, yI minus a, x, I, x, I, b, times the derivative of this, we respect to a.  to a negative xI, and so we'll want this to be 0. And partial d of a, partial b, we do the same thing, but differentiating with respect to b, instead of
respect to a, so again, sum of squares, twice yI minus a, xI plus b, times the derivative of b's, we respect to b is, I think,  so that the equations we have to solve, well, let's reorganize this a little bit.  So c, there's a's and b's in these equations, I'm going to just look at the coefficients of a and b. If you have good I's, you can see,
probably that these are actually linear equations in a and b, or for there's a lot of clutter with all these x's and y's, all of other place.  vectors of 2, that are just not very important, I can simplify things by 2. And next, I'm going to look at the
equalition of a, well, I will get, basically, a times x i squared, well, let me just do it, and it should be clear.  plus xI times b minus xIyI, and we set this equal to 0.  be a 1. So we just multiply by minus 1, so we take the opposite of f, it
will be axy plus b, and we'll find that as x x, x i, a, plus b minus y i. And so I forgot the n. And let me just reorganize that by actually  all the a's together, that means actually I will have sum of all the xI squared times a plus sum of xI
times b minus sum of xIyI equals 0.  So if I rewrite this, it becomes sum of x i squared times a plus sum of the x i's times b minus 1.  And that one becomes sum of x i times a plus how many b's do I get from this one? I get 1 for each
data point, when I sum them together, I will get n, so n times b equals sum of y. Now, this quantity is, it looks scary, but there actually just numbers,  for this one, you look at all your data points for each of them, you take a
value of x, and you just sum all these numbers together. So what you get, actually, is a linear system in a and b, a 2 by 2, linear system.  the enemy. So in practice, of course, first you plug in the numbers for x, I, and y, and then you solve the
sub-sub n that you get, and we know how to solve 2 by 2 linear systems, I hope. So that's how we find the best f at 9. Now, y is that going to be the best one instead of a worst one, which is a sub-thorough thought at a equal point, that could actually be a maximum of this  and so we'll have actually have the answer to that next time, but trust me, if you really want to go for the
second derivative test that we'll see to mow and apply it in this case, it's quite hard to check, but you can see it's actually a minimum.  Now, even further, in the R case is the one that we have a most
family on with, List square's interpolation actually works in much more general settings.  because instead of fitting for the x 9, if you think that there's a
different kind of relation, then maybe you can fit in using a different kind of formula. So let me actually illustrate that with an example. So I don't know if you're familiar with Mose-Law, it's something that's supposed to tell you how quickly, basically, compute your chips become smarter, faster and faster over time.  that you can fit onto a computer chip. So here I have some
data about, why? So here's data about a number of transistors on a stand-up PC processor as a function of time, and if you try to do the best line  it, well, you'll see quickly, but it doesn't seem to
follow a linear trend. On the other hand, if you plug the diagram in a log scale with the log of a number of transistors as a function of time, then you get a much better line.  also, it says it says that the number of transistors and the chip doubles every, so
depending on the version, every 18 months or every two years, they keep changing the statement.  y equals a constant times x
ponential of a times x, that's what we want to look at. What we could try to minimize a square error or an acquitted before, that doesn't work well at all, the equations that you get are very complicated, you can't solve them.  it becomes a linear relation. So, observe, this is the same as ln of y equals ln of c plus ax, and that is a linear
best fit. So what you do is you just look for the best straight line fit for  of a log of y. So that's something we already know. But you can also do, for example, let's say that we have something more complicated, let's say that we have a
quadratic, so for example, y is of a form a x squared plus b, x plus c. And of course, you're trying to find somehow  So that would mean here, fitting the best
parabola for your data points, well, to do that you would need to find a, b, and c. And now you would have actually a function of a, b, and c, which would be the sum of all data points of the square deviation.  So now you will have free equations involving a, b, and c, and in fact, you will find a free by free
between the linear system, and it works just the same way, just you have a little bit more data. So basically, you see that this best-fidth problem, an example of a minimum of a minimum of
minimization problem that maybe you didn't expect to see, a minimum of
minimization
problems come in, but that's really the way to handle these questions.  OK, so tomorrow we'll go back to the question of how do we decide whether it's a minimum or a
minimum or a maximum and we'll continue exploring some terms of several variables?